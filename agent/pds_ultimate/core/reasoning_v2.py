"""
PDS-Ultimate Internet Reasoning Layer v2 (Part 8)
====================================================
–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π —Å–ª–æ–π –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-—Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π.

–£—Å–∏–ª–∏–≤–∞–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π Internet Reasoning Engine (Part 5):

1. Multi-Source Verification ‚Äî –º–∏–Ω–∏–º—É–º 2-3 –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞
2. Trust Scoring v2 ‚Äî –æ—Ü–µ–Ω–∫–∞ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏ —Å–∞–π—Ç–æ–≤ –ø–æ –∏—Å—Ç–æ—Ä–∏–∏
3. Contradiction Detection ‚Äî –≤—ã—è–≤–ª–µ–Ω–∏–µ —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–π –º–µ–∂–¥—É –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏
4. Staleness Detection ‚Äî –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö
5. Source Credibility ‚Äî –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∫–æ–º—É –≤–µ—Ä–∏—Ç—å –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ
6. Self-Query Expansion ‚Äî –∞–≥–µ–Ω—Ç —Å–∞–º —É—Ç–æ—á–Ω—è–µ—Ç –∑–∞–ø—Ä–æ—Å
7. Hypothesis Testing ‚Äî –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –≥–∏–ø–æ—Ç–µ–∑
8. Fact Extraction ‚Äî –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ñ–∞–∫—Ç–æ–≤
9. Confidence Calibration ‚Äî –∫–∞–ª–∏–±—Ä–æ–≤–∫–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –ø–æ –¥–∞–Ω–Ω—ã–º
10. Context Compression ‚Äî —Å–∂–∞—Ç–∏–µ –±–æ–ª—å—à–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤ –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ —Å–º—ã—Å–ª–∞
"""

from __future__ import annotations

import re
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from typing import Any

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# TRUST SCORER v2 ‚Äî –û—Ü–µ–Ω–∫–∞ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê


class TrustScorerV2:
    """
    –û—Ü–µ–Ω–∫–∞ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤.

    –§–∞–∫—Ç–æ—Ä—ã:
    - –î–æ–º–µ–Ω (Wikipedia = –≤—ã—Å–æ–∫–∏–π, random blog = –Ω–∏–∑–∫–∏–π)
    - –í–æ–∑—Ä–∞—Å—Ç –∫–æ–Ω—Ç–µ–Ω—Ç–∞ (—Å–≤–µ–∂–∏–π = –≤—ã—à–µ)
    - –ù–∞–ª–∏—á–∏–µ —Å—Å—ã–ª–æ–∫/—Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π
    - –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å –¥—Ä—É–≥–∏–º–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏ (consensus)
    - –ò—Å—Ç–æ—Ä–∏—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π (–µ—Å–ª–∏ —Ä–∞–Ω—å—à–µ –¥–∞–≤–∞–ª –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ)
    """

    # –ë–∞–∑–æ–≤—ã–µ trust scores –ø–æ –¥–æ–º–µ–Ω–∞–º
    DOMAIN_SCORES: dict[str, float] = {
        # –í—ã—Å–æ–∫–∞—è –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å (0.8-1.0)
        "wikipedia.org": 0.90,
        "britannica.com": 0.92,
        "gov": 0.88,  # .gov –¥–æ–º–µ–Ω—ã
        "edu": 0.85,  # .edu –¥–æ–º–µ–Ω—ã
        "nature.com": 0.95,
        "science.org": 0.94,
        "pubmed.ncbi.nlm.nih.gov": 0.93,
        "reuters.com": 0.88,
        "bbc.com": 0.85,
        "bbc.co.uk": 0.85,
        "nytimes.com": 0.83,

        # –°—Ä–µ–¥–Ω—è—è –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å (0.5-0.8)
        "stackoverflow.com": 0.80,
        "github.com": 0.75,
        "medium.com": 0.55,
        "quora.com": 0.50,
        "reddit.com": 0.45,

        # –ù–∏–∑–∫–∞—è –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å (0.1-0.5)
        "twitter.com": 0.35,
        "x.com": 0.35,
        "facebook.com": 0.30,
        "tiktok.com": 0.20,
    }

    # –î–æ–º–µ–Ω—ã-—Å–ø–∞–º–µ—Ä—ã (–≤—Å–µ–≥–¥–∞ –Ω–∏–∑–∫–∏–π trust)
    SPAM_DOMAINS = frozenset({
        "clickbait", "ads", "spam", "fake",
    })

    def __init__(self):
        self._history: dict[str, list[float]] = {}  # domain ‚Üí [scores]
        self._custom_scores: dict[str, float] = {}

    def score_domain(self, url: str) -> float:
        """–û—Ü–µ–Ω–∏—Ç—å –¥–æ–º–µ–Ω –ø–æ URL."""
        domain = self._extract_domain(url)

        # –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞
        if domain in self._custom_scores:
            return self._custom_scores[domain]

        # –ü–æ –±–∞–∑–µ
        for known_domain, score in self.DOMAIN_SCORES.items():
            if known_domain in domain:
                return score

        # .gov, .edu –¥–æ–º–µ–Ω—ã
        if domain.endswith(".gov"):
            return 0.88
        if domain.endswith(".edu"):
            return 0.85

        # –ò—Å—Ç–æ—Ä–∏—è
        if domain in self._history:
            scores = self._history[domain]
            if scores:
                return sum(scores) / len(scores)

        # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
        return 0.50

    def score_content(
        self,
        text: str,
        url: str = "",
        publish_date: datetime | None = None,
    ) -> float:
        """
        –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞.

        –£—á–∏—Ç—ã–≤–∞–µ—Ç: –¥–æ–º–µ–Ω + —Å–≤–µ–∂–µ—Å—Ç—å + –¥–ª–∏–Ω—É + –Ω–∞–ª–∏—á–∏–µ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π.
        """
        score = self.score_domain(url)

        # –°–≤–µ–∂–µ—Å—Ç—å (-0.1 –∑–∞ –∫–∞–∂–¥—ã–π –≥–æ–¥ —Å—Ç–∞—Ä–æ—Å—Ç–∏)
        if publish_date:
            age_days = (datetime.utcnow() - publish_date).days
            if age_days > 365 * 3:
                score *= 0.7  # –ë–æ–ª–µ–µ 3 –ª–µ—Ç ‚Äî —Å–Ω–∏–∂–∞–µ–º
            elif age_days > 365:
                score *= 0.85
            elif age_days > 180:
                score *= 0.95

        # –î–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ (—Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π = –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ)
        if len(text) < 100:
            score *= 0.7
        elif len(text) < 500:
            score *= 0.85

        # –ù–∞–ª–∏—á–∏–µ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π/—Å—Å—ã–ª–æ–∫
        citations = text.count(
            "[") + text.count("source") + text.count("according to")
        if citations > 3:
            score = min(1.0, score * 1.1)

        return min(1.0, max(0.0, score))

    def update_history(self, url: str, score: float) -> None:
        """–û–±–Ω–æ–≤–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é –¥–ª—è –¥–æ–º–µ–Ω–∞."""
        domain = self._extract_domain(url)
        if domain not in self._history:
            self._history[domain] = []
        self._history[domain].append(score)
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é
        if len(self._history[domain]) > 50:
            self._history[domain] = self._history[domain][-25:]

    def set_custom_score(self, domain: str, score: float) -> None:
        """–£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫—É—é –æ—Ü–µ–Ω–∫—É –¥–æ–º–µ–Ω–∞."""
        self._custom_scores[domain] = max(0.0, min(1.0, score))

    @staticmethod
    def _extract_domain(url: str) -> str:
        """–ò–∑–≤–ª–µ—á—å –¥–æ–º–µ–Ω –∏–∑ URL."""
        url = url.lower().strip()
        # –£–±–∏—Ä–∞–µ–º –ø—Ä–æ—Ç–æ–∫–æ–ª
        for prefix in ["https://", "http://", "www."]:
            if url.startswith(prefix):
                url = url[len(prefix):]
        # –ë–µ—Ä—ë–º –¥–æ –ø–µ—Ä–≤–æ–≥–æ /
        return url.split("/")[0].split("?")[0]


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# CONTRADICTION DETECTOR ‚Äî –í—ã—è–≤–ª–µ–Ω–∏–µ —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–π
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê


@dataclass
class FactClaim:
    """–§–∞–∫—Ç/—É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–∞."""
    text: str
    source_url: str = ""
    source_name: str = ""
    trust_score: float = 0.5
    timestamp: datetime = field(default_factory=datetime.utcnow)
    category: str = ""  # number, date, name, statement

    def to_dict(self) -> dict:
        return {
            "text": self.text,
            "source": self.source_name or self.source_url,
            "trust": round(self.trust_score, 2),
            "category": self.category,
        }


@dataclass
class Contradiction:
    """–û–±–Ω–∞—Ä—É–∂–µ–Ω–Ω–æ–µ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–µ."""
    claim_a: FactClaim
    claim_b: FactClaim
    description: str = ""
    severity: str = "medium"  # low, medium, high
    resolution: str = ""  # –ö–∞–∫–æ–º—É –∏—Å—Ç–æ—á–Ω–∏–∫—É –≤–µ—Ä–∏—Ç—å

    def to_dict(self) -> dict:
        return {
            "claim_a": self.claim_a.to_dict(),
            "claim_b": self.claim_b.to_dict(),
            "description": self.description,
            "severity": self.severity,
            "resolution": self.resolution,
        }


class ContradictionDetector:
    """
    –î–µ—Ç–µ–∫—Ç–æ—Ä –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–π –º–µ–∂–¥—É –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏.

    –°—Ç—Ä–∞—Ç–µ–≥–∏–∏:
    - –ß–∏—Å–ª–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ: –ø—Ä—è–º–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ
    - –î–∞—Ç—ã: —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä–∞–º–æ–∫
    - –£—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è: —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ (keywords overlap)
    """

    def detect(self, facts: list[FactClaim]) -> list[Contradiction]:
        """–û–±–Ω–∞—Ä—É–∂–∏—Ç—å –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è –≤ —Å–ø–∏—Å–∫–µ —Ñ–∞–∫—Ç–æ–≤."""
        contradictions: list[Contradiction] = []

        for i in range(len(facts)):
            for j in range(i + 1, len(facts)):
                contradiction = self._compare_facts(facts[i], facts[j])
                if contradiction:
                    contradictions.append(contradiction)

        return contradictions

    def _compare_facts(self, a: FactClaim, b: FactClaim) -> Contradiction | None:
        """–°—Ä–∞–≤–Ω–∏—Ç—å –¥–≤–∞ —Ñ–∞–∫—Ç–∞ –Ω–∞ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–µ."""
        # –û–¥–∏–Ω –∏—Å—Ç–æ—á–Ω–∏–∫ ‚Äî –Ω–µ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–µ
        if a.source_url == b.source_url:
            return None

        # –ß–∏—Å–ª–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
        nums_a = self._extract_numbers(a.text)
        nums_b = self._extract_numbers(b.text)

        if nums_a and nums_b:
            # –ï—Å–ª–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø–æ—Ö–æ–∂–∏–π –Ω–æ —á–∏—Å–ª–∞ —Ä–∞–∑–Ω—ã–µ
            similarity = self._text_similarity(a.text, b.text)
            if similarity > 0.3:
                for na in nums_a:
                    for nb in nums_b:
                        if na != 0 and nb != 0:
                            diff = abs(na - nb) / max(abs(na), abs(nb))
                            if diff > 0.15:  # >15% —Ä–∞–∑–Ω–∏—Ü–∞
                                # –ö–æ–º—É –≤–µ—Ä–∏—Ç—å?
                                resolution = ""
                                if a.trust_score > b.trust_score + 0.1:
                                    resolution = f"–î–æ–≤–µ—Ä—è–µ–º: {a.source_name or a.source_url}"
                                elif b.trust_score > a.trust_score + 0.1:
                                    resolution = f"–î–æ–≤–µ—Ä—è–µ–º: {b.source_name or b.source_url}"
                                else:
                                    resolution = "–ù—É–∂–Ω–∞ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞"

                                severity = "high" if diff > 0.5 else "medium" if diff > 0.25 else "low"

                                return Contradiction(
                                    claim_a=a,
                                    claim_b=b,
                                    description=(
                                        f"–ß–∏—Å–ª–æ–≤–æ–µ —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–µ: {na} vs {nb} "
                                        f"(—Ä–∞–∑–Ω–∏—Ü–∞ {diff:.0%})"
                                    ),
                                    severity=severity,
                                    resolution=resolution,
                                )

        # –ü—Ä—è–º—ã–µ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        negation_pairs = [
            ("yes", "no"), ("–¥–∞", "–Ω–µ—Ç"),
            ("true", "false"), ("–≤–µ—Ä–Ω–æ", "–Ω–µ–≤–µ—Ä–Ω–æ"),
            ("increased", "decreased"), ("–≤—ã—Ä–æ—Å–ª", "—É–ø–∞–ª"),
            ("support", "oppose"), ("–∑–∞", "–ø—Ä–æ—Ç–∏–≤"),
        ]

        a_lower = a.text.lower()
        b_lower = b.text.lower()

        for pos, neg in negation_pairs:
            if (pos in a_lower and neg in b_lower) or (neg in a_lower and pos in b_lower):
                similarity = self._text_similarity(a.text, b.text)
                if similarity > 0.2:
                    return Contradiction(
                        claim_a=a,
                        claim_b=b,
                        description=f"–ü—Ä–æ—Ç–∏–≤–æ–ø–æ–ª–æ–∂–Ω—ã–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è: {pos}/{neg}",
                        severity="medium",
                        resolution=self._resolve_by_trust(a, b),
                    )

        return None

    def _resolve_by_trust(self, a: FactClaim, b: FactClaim) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–æ–º—É –≤–µ—Ä–∏—Ç—å –ø–æ trust score."""
        if a.trust_score > b.trust_score + 0.15:
            return f"–î–æ–≤–µ—Ä—è–µ–º: {a.source_name or '–∏—Å—Ç–æ—á–Ω–∏–∫ A'} (trust={a.trust_score:.2f})"
        elif b.trust_score > a.trust_score + 0.15:
            return f"–î–æ–≤–µ—Ä—è–µ–º: {b.source_name or '–∏—Å—Ç–æ—á–Ω–∏–∫ B'} (trust={b.trust_score:.2f})"
        return "–†–∞–≤–Ω–∞—è –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å ‚Äî –Ω—É–∂–Ω–∞ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞"

    @staticmethod
    def _extract_numbers(text: str) -> list[float]:
        """–ò–∑–≤–ª–µ—á—å —á–∏—Å–ª–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞."""
        numbers = re.findall(r'-?\d+\.?\d*', text)
        return [float(n) for n in numbers[:5]]

    @staticmethod
    def _text_similarity(a: str, b: str) -> float:
        """–ü—Ä–æ—Å—Ç–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ —Ç–µ–∫—Å—Ç–æ–≤ (Jaccard –ø–æ —Å–ª–æ–≤–∞–º)."""
        words_a = set(re.findall(r'\w{3,}', a.lower()))
        words_b = set(re.findall(r'\w{3,}', b.lower()))

        if not words_a or not words_b:
            return 0.0

        intersection = words_a & words_b
        union = words_a | words_b

        return len(intersection) / len(union) if union else 0.0


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# QUERY EXPANDER ‚Äî –°–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ–µ —É—Ç–æ—á–Ω–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–æ–≤
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê


class QueryExpander:
    """
    –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∏ —É—Ç–æ—á–Ω–µ–Ω–∏–µ –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤.

    –ê–≥–µ–Ω—Ç –Ω–µ –ø—Ä–æ—Å—Ç–æ –∏—â–µ—Ç 1 —Ä–∞–∑ ‚Äî –æ–Ω:
    1. –ò—â–µ—Ç –ø–æ –∏—Å—Ö–æ–¥–Ω–æ–º—É –∑–∞–ø—Ä–æ—Å—É
    2. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã ‚Äî —á–µ–≥–æ –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç?
    3. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —É—Ç–æ—á–Ω—ë–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
    4. –ò—â–µ—Ç –ø–æ–≤—Ç–æ—Ä–Ω–æ
    5. –°–∏–Ω—Ç–µ–∑–∏—Ä—É–µ—Ç

    Self-Query Expansion:
    - –°–∏–Ω–æ–Ω–∏–º—ã –∏ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏
    - –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã (site:, intitle:)
    - –ü–µ—Ä–µ–≤–æ–¥ –Ω–∞ –¥—Ä—É–≥–∏–µ —è–∑—ã–∫–∏ –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –æ—Ö–≤–∞—Ç–∞
    """

    # –ü–∞—Ç—Ç–µ—Ä–Ω—ã —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è
    EXPANSION_PATTERNS: dict[str, list[str]] = {
        "—Ü–µ–Ω–∞|—Å—Ç–æ–∏–º–æ—Å—Ç—å|price|cost": [
            "{query} price comparison",
            "{query} —Å—Ç–æ–∏–º–æ—Å—Ç—å 2026",
            "{query} —Ü–µ–Ω–∞ –æ—Ç–∑—ã–≤—ã",
        ],
        "–ª—É—á—à–∏–π|best|top": [
            "{query} comparison",
            "{query} vs alternatives",
            "{query} —Ä–µ–π—Ç–∏–Ω–≥",
        ],
        "–∫–∞–∫|how|tutorial": [
            "{query} tutorial step by step",
            "{query} guide for beginners",
            "{query} –ø—Ä–∏–º–µ—Ä—ã",
        ],
        "–Ω–æ–≤–æ—Å—Ç|news|update": [
            "{query} latest news 2026",
            "{query} –ø–æ—Å–ª–µ–¥–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏",
        ],
        "–æ—à–∏–±–∫–∞|error|problem|bug": [
            "{query} fix solution",
            "{query} how to solve",
            "{query} workaround",
        ],
    }

    def expand(self, query: str, max_queries: int = 3) -> list[str]:
        """
        –†–∞—Å—à–∏—Ä–∏—Ç—å –∑–∞–ø—Ä–æ—Å –≤–∞—Ä–∏–∞–Ω—Ç–∞–º–∏.

        Returns:
            –°–ø–∏—Å–æ–∫ –∑–∞–ø—Ä–æ—Å–æ–≤ (–≤–∫–ª—é—á–∞—è –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π)
        """
        queries = [query]

        lower = query.lower()

        # –ü–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º
        for pattern, templates in self.EXPANSION_PATTERNS.items():
            if re.search(pattern, lower):
                for template in templates[:max_queries - 1]:
                    expanded = template.format(query=query)
                    if expanded not in queries:
                        queries.append(expanded)

        # –ï—Å–ª–∏ –º–∞–ª–æ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ ‚Äî –¥–æ–±–∞–≤–ª—è–µ–º –±–∞–∑–æ–≤—ã–µ
        if len(queries) < max_queries:
            # –î–æ–±–∞–≤–ª—è–µ–º –≥–æ–¥ –¥–ª—è –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç–∏
            if "2026" not in query and "2025" not in query:
                queries.append(f"{query} 2026")

            # –î–æ–±–∞–≤–ª—è–µ–º "site:" –¥–ª—è –∞–≤—Ç–æ—Ä–∏—Ç–µ—Ç–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
            if len(queries) < max_queries:
                queries.append(
                    f"{query} site:wikipedia.org OR site:britannica.com")

        return queries[:max_queries]

    def refine_from_results(
        self,
        original_query: str,
        results_summary: str,
        gaps: list[str] | None = None,
    ) -> list[str]:
        """
        –£—Ç–æ—á–Ω–∏—Ç—å –∑–∞–ø—Ä–æ—Å –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.

        Args:
            original_query: –ò—Å—Ö–æ–¥–Ω—ã–π –∑–∞–ø—Ä–æ—Å
            results_summary: –ß—Ç–æ —É–∂–µ –Ω–∞—à–ª–∏
            gaps: –ß—Ç–æ –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç

        Returns:
            –£—Ç–æ—á–Ω—ë–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
        """
        refined = []

        if gaps:
            for gap in gaps[:3]:
                refined.append(f"{original_query} {gap}")

        # –ï—Å–ª–∏ –µ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã ‚Äî –∏—â–µ–º –∫–æ–Ω–∫—Ä–µ—Ç–Ω–µ–µ
        if results_summary:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Ç–µ—Ä–º–∏–Ω—ã –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            terms = re.findall(r'\b[A-Z–ê-–Ø][a-z–∞-—è]{3,}\b', results_summary)
            unique_terms = list(set(terms))[:3]
            if unique_terms:
                refined.append(f"{original_query} {' '.join(unique_terms)}")

        return refined if refined else [original_query]


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# HYPOTHESIS TESTER ‚Äî –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –≥–∏–ø–æ—Ç–µ–∑
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê


@dataclass
class Hypothesis:
    """–ì–∏–ø–æ—Ç–µ–∑–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏."""
    id: str = ""
    statement: str = ""
    confidence: float = 0.5
    evidence_for: list[FactClaim] = field(default_factory=list)
    evidence_against: list[FactClaim] = field(default_factory=list)
    status: str = "testing"  # testing, confirmed, rejected, uncertain
    tested_at: datetime | None = None

    @property
    def net_evidence(self) -> float:
        """–ß–∏—Å—Ç–∞—è –æ—Ü–µ–Ω–∫–∞: (–∑–∞ - –ø—Ä–æ—Ç–∏–≤) / –≤—Å–µ–≥–æ."""
        total = len(self.evidence_for) + len(self.evidence_against)
        if total == 0:
            return 0.0

        score_for = sum(e.trust_score for e in self.evidence_for)
        score_against = sum(e.trust_score for e in self.evidence_against)

        return (score_for - score_against) / total

    def to_dict(self) -> dict:
        return {
            "statement": self.statement,
            "confidence": round(self.confidence, 2),
            "evidence_for": len(self.evidence_for),
            "evidence_against": len(self.evidence_against),
            "net_evidence": round(self.net_evidence, 2),
            "status": self.status,
        }


class HypothesisTester:
    """
    –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –≥–∏–ø–æ—Ç–µ–∑.

    1. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≥–∏–ø–æ—Ç–µ–∑—ã –∏–∑ –≤–æ–ø—Ä–æ—Å–∞
    2. –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–∞–∂–¥—É—é —á–µ—Ä–µ–∑ –ø–æ–∏—Å–∫
    3. –°–æ–±–∏—Ä–∞–µ—Ç evidence for/against
    4. –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –Ω–∞–∏–±–æ–ª–µ–µ –≤–µ—Ä–æ—è—Ç–Ω—ã–π –æ—Ç–≤–µ—Ç
    """

    def generate_hypotheses(self, question: str, max_count: int = 3) -> list[Hypothesis]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≥–∏–ø–æ—Ç–µ–∑ –∏–∑ –≤–æ–ø—Ä–æ—Å–∞ (rule-based)."""
        hypotheses: list[Hypothesis] = []

        lower = question.lower()

        # –ë–∏–Ω–∞—Ä–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã
        if any(w in lower for w in ["–ª–∏", "–º–æ–∂–Ω–æ –ª–∏", "is it", "can", "should"]):
            hypotheses.append(Hypothesis(
                id="h_yes",
                statement=f"–î–ê: {question}",
                confidence=0.5,
            ))
            hypotheses.append(Hypothesis(
                id="h_no",
                statement=f"–ù–ï–¢: {question}",
                confidence=0.5,
            ))

        # –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã
        elif any(w in lower for w in ["–ª—É—á—à–µ", "better", "vs", "–∏–ª–∏", " or "]):
            parts = re.split(r'\b–∏–ª–∏\b|\bor\b|\bvs\b|\b–ª—É—á—à–µ\b', lower)
            if len(parts) >= 2:
                for i, part in enumerate(parts[:max_count]):
                    hypotheses.append(Hypothesis(
                        id=f"h_{i}",
                        statement=f"–õ—É—á—à–∏–π –≤–∞—Ä–∏–∞–Ω—Ç: {part.strip()}",
                        confidence=0.5,
                    ))

        # –û–±—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã ‚Äî –æ–¥–Ω–∞ –≥–∏–ø–æ—Ç–µ–∑–∞
        if not hypotheses:
            hypotheses.append(Hypothesis(
                id="h_main",
                statement=question,
                confidence=0.5,
            ))

        return hypotheses[:max_count]

    def evaluate_hypothesis(
        self,
        hypothesis: Hypothesis,
        facts: list[FactClaim],
    ) -> Hypothesis:
        """–û—Ü–µ–Ω–∏—Ç—å –≥–∏–ø–æ—Ç–µ–∑—É –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ñ–∞–∫—Ç–æ–≤."""
        for fact in facts:
            # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞: –µ—Å–ª–∏ —Ñ–∞–∫—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –≥–∏–ø–æ—Ç–µ–∑—ã
            h_words = set(re.findall(r'\w{3,}', hypothesis.statement.lower()))
            f_words = set(re.findall(r'\w{3,}', fact.text.lower()))

            overlap = len(h_words & f_words) / max(len(h_words), 1)

            if overlap > 0.3:
                # –§–∞–∫—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–µ–Ω –≥–∏–ø–æ—Ç–µ–∑–µ
                # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∏–ª–∏ –æ–ø—Ä–æ–≤–µ—Ä–≥–∞–µ—Ç?
                negative_words = {
                    "–Ω–µ", "–Ω–µ—Ç", "–Ω–µ–ª—å–∑—è", "–Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ", "–æ—à–∏–±–æ—á–Ω–æ",
                    "not", "no", "cannot", "impossible", "wrong", "false",
                }

                is_negative = any(w in fact.text.lower()
                                  for w in negative_words)

                if is_negative:
                    hypothesis.evidence_against.append(fact)
                else:
                    hypothesis.evidence_for.append(fact)

        # –û–±–Ω–æ–≤–ª—è–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
        net = hypothesis.net_evidence
        hypothesis.confidence = max(0.0, min(1.0, 0.5 + net * 0.5))

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç–∞—Ç—É—Å
        if hypothesis.confidence > 0.75:
            hypothesis.status = "confirmed"
        elif hypothesis.confidence < 0.25:
            hypothesis.status = "rejected"
        else:
            hypothesis.status = "uncertain"

        hypothesis.tested_at = datetime.utcnow()
        return hypothesis


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# CONTEXT COMPRESSOR ‚Äî –°–∂–∞—Ç–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê


class ContextCompressor:
    """
    –°–∂–∞—Ç–∏–µ –±–æ–ª—å—à–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ LLM.

    –°—Ç—Ä–∞—Ç–µ–≥–∏–∏:
    - Extractive: –≤—ã–±–æ—Ä —Å–∞–º—ã—Ö –≤–∞–∂–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
    - Chunking: —Ä–∞–∑–±–∏–≤–∫–∞ –Ω–∞ —Å–º—ã—Å–ª–æ–≤—ã–µ –±–ª–æ–∫–∏
    - Deduplication: —É–¥–∞–ª–µ–Ω–∏–µ –ø–æ–≤—Ç–æ—Ä–æ–≤
    """

    def compress(
        self,
        text: str,
        max_length: int = 2000,
        strategy: str = "extractive",
    ) -> str:
        """
        –°–∂–∞—Ç—å —Ç–µ–∫—Å—Ç –¥–æ max_length —Å–∏–º–≤–æ–ª–æ–≤.

        Strategies:
        - extractive: –≤—ã–±–æ—Ä –∫–ª—é—á–µ–≤—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        - truncate: –ø—Ä–æ—Å—Ç–æ–µ –æ–±—Ä–µ–∑–∞–Ω–∏–µ
        - smart: extractive + dedup
        """
        if len(text) <= max_length:
            return text

        if strategy == "truncate":
            return text[:max_length] + "..."

        if strategy == "extractive" or strategy == "smart":
            return self._extractive_compress(text, max_length)

        return text[:max_length]

    def _extractive_compress(self, text: str, max_length: int) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π."""
        sentences = self._split_sentences(text)
        if not sentences:
            return text[:max_length]

        # –û—Ü–µ–Ω–∏–≤–∞–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å –∫–∞–∂–¥–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        scored = []
        for i, sent in enumerate(sentences):
            score = self._sentence_importance(sent, i, len(sentences))
            scored.append((score, i, sent))

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏
        scored.sort(key=lambda x: x[0], reverse=True)

        # –°–æ–±–∏—Ä–∞–µ–º –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö –ª–∏–º–∏—Ç–∞ (–≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ)
        selected: list[tuple[int, str]] = []
        current_length = 0

        for score, idx, sent in scored:
            if current_length + len(sent) + 1 <= max_length:
                selected.append((idx, sent))
                current_length += len(sent) + 1

        # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–æ—Ä—è–¥–æ–∫
        selected.sort(key=lambda x: x[0])

        return " ".join(s for _, s in selected)

    def _sentence_importance(
        self,
        sentence: str,
        position: int,
        total: int,
    ) -> float:
        """–û—Ü–µ–Ω–∫–∞ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è."""
        score = 0.0

        # –ü–æ–∑–∏—Ü–∏—è (–Ω–∞—á–∞–ª–æ –∏ –∫–æ–Ω–µ—Ü –≤–∞–∂–Ω–µ–µ)
        if position < total * 0.2:
            score += 0.3  # –ù–∞—á–∞–ª–æ —Ç–µ–∫—Å—Ç–∞
        elif position > total * 0.8:
            score += 0.2  # –ö–æ–Ω–µ—Ü —Ç–µ–∫—Å—Ç–∞

        # –î–ª–∏–Ω–∞ (—Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ ‚Äî –Ω–µ–∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ)
        length = len(sentence)
        if length > 50:
            score += 0.1
        if length > 100:
            score += 0.1
        if length < 20:
            score -= 0.2

        # –°–æ–¥–µ—Ä–∂–∏—Ç —á–∏—Å–ª–∞ (—Ñ–∞–∫—Ç—ã)
        if re.search(r'\d', sentence):
            score += 0.2

        # –°–æ–¥–µ—Ä–∂–∏—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        important_words = {
            "–≤–∞–∂–Ω–æ", "–∫–ª—é—á–µ–≤", "–≥–ª–∞–≤–Ω", "–∏—Ç–æ–≥–æ", "–≤—ã–≤–æ–¥", "—Ä–µ–∑—É–ª—å—Ç–∞—Ç",
            "important", "key", "main", "result", "conclusion", "total",
            "therefore", "however", "specifically", "notably",
        }
        lower = sentence.lower()
        for w in important_words:
            if w in lower:
                score += 0.15
                break

        # –°–æ–¥–µ—Ä–∂–∏—Ç –∏–º–µ–Ω–∞ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ (–∑–∞–≥–ª–∞–≤–Ω—ã–µ –±—É–∫–≤—ã –≤ —Å–µ—Ä–µ–¥–∏–Ω–µ)
        if re.search(r'\b[A-Z–ê-–Ø][a-z–∞-—è]+\s[A-Z–ê-–Ø]', sentence):
            score += 0.1

        return score

    def _split_sentences(self, text: str) -> list[str]:
        """–†–∞–∑–±–∏—Ç—å —Ç–µ–∫—Å—Ç –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è."""
        # –ü—Ä–æ—Å—Ç–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –ø–æ . ! ?
        sentences = re.split(r'(?<=[.!?])\s+', text)
        return [s.strip() for s in sentences if len(s.strip()) > 10]

    def deduplicate(self, texts: list[str]) -> list[str]:
        """–£–¥–∞–ª–∏—Ç—å –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ –æ—á–µ–Ω—å –ø–æ—Ö–æ–∂–∏–µ —Ç–µ–∫—Å—Ç—ã."""
        if not texts:
            return []

        unique: list[str] = [texts[0]]

        for text in texts[1:]:
            is_dup = False
            for existing in unique:
                sim = ContradictionDetector._text_similarity(text, existing)
                if sim > 0.7:
                    is_dup = True
                    break
            if not is_dup:
                unique.append(text)

        return unique

    def chunk(self, text: str, chunk_size: int = 1000, overlap: int = 100) -> list[str]:
        """–†–∞–∑–±–∏—Ç—å —Ç–µ–∫—Å—Ç –Ω–∞ chunks —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º."""
        if len(text) <= chunk_size:
            return [text]

        chunks = []
        start = 0

        while start < len(text):
            end = start + chunk_size

            # –ò—â–µ–º –∫–æ–Ω–µ—Ü –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
            if end < len(text):
                # –ò—â–µ–º –±–ª–∏–∂–∞–π—à—É—é —Ç–æ—á–∫—É
                dot_pos = text.rfind(".", start + chunk_size // 2, end + 100)
                if dot_pos > start:
                    end = dot_pos + 1

            chunks.append(text[start:end].strip())
            start = end - overlap

        return chunks


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# CONFIDENCE ENGINE ‚Äî –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –∏ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ—Å—Ç—å
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê


@dataclass
class ConfidenceAssessment:
    """–û—Ü–µ–Ω–∫–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –≤ –æ—Ç–≤–µ—Ç–µ."""
    overall: float = 0.5           # 0.0 - 1.0
    source_quality: float = 0.5    # –ö–∞—á–µ—Å—Ç–≤–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
    consensus: float = 0.5         # –°–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
    freshness: float = 0.5         # –ê–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö
    completeness: float = 0.5      # –ü–æ–ª–Ω–æ—Ç–∞ –æ—Ç–≤–µ—Ç–∞
    needs_more_data: bool = False   # –ù—É–∂–Ω–æ –±–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö?
    gaps: list[str] = field(default_factory=list)  # –ß—Ç–æ –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç

    @property
    def label(self) -> str:
        if self.overall >= 0.8:
            return "üü¢ –í—ã—Å–æ–∫–∞—è"
        elif self.overall >= 0.6:
            return "üü° –°—Ä–µ–¥–Ω—è—è"
        elif self.overall >= 0.4:
            return "üü† –ù–∏–∑–∫–∞—è"
        else:
            return "üî¥ –û—á–µ–Ω—å –Ω–∏–∑–∫–∞—è"

    def to_dict(self) -> dict:
        return {
            "overall": round(self.overall, 2),
            "label": self.label,
            "source_quality": round(self.source_quality, 2),
            "consensus": round(self.consensus, 2),
            "freshness": round(self.freshness, 2),
            "completeness": round(self.completeness, 2),
            "needs_more_data": self.needs_more_data,
            "gaps": self.gaps,
        }


class ConfidenceEngine:
    """
    –î–≤–∏–∂–æ–∫ –æ—Ü–µ–Ω–∫–∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏.

    –ï—Å–ª–∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –Ω–∏–∑–∫–∞—è ‚Üí –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫.
    """

    THRESHOLD_LOW = 0.4        # –ù–∏–∂–µ ‚Äî –Ω—É–∂–µ–Ω –¥–æ–ø–æ–∏—Å–∫
    THRESHOLD_ACCEPTABLE = 0.6  # –í—ã—à–µ ‚Äî –º–æ–∂–Ω–æ –æ—Ç–≤–µ—á–∞—Ç—å

    def assess(
        self,
        facts: list[FactClaim],
        contradictions: list[Contradiction] | None = None,
        query: str = "",
    ) -> ConfidenceAssessment:
        """–û—Ü–µ–Ω–∏—Ç—å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ —Å–æ–±—Ä–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö."""
        if not facts:
            return ConfidenceAssessment(
                overall=0.1,
                needs_more_data=True,
                gaps=["–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö"],
            )

        # Source quality
        trust_scores = [f.trust_score for f in facts]
        source_quality = sum(trust_scores) / len(trust_scores)

        # Consensus (—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å)
        contradictions = contradictions or []
        if len(facts) > 1:
            consensus = 1.0 - (len(contradictions) / max(len(facts), 1))
            consensus = max(0.0, consensus)
        else:
            consensus = 0.5  # –û–¥–∏–Ω –∏—Å—Ç–æ—á–Ω–∏–∫ ‚Äî —Å—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å

        # Freshness (–±–µ—Ä—ë–º —Å—Ä–µ–¥–Ω–∏–π –≤–æ–∑—Ä–∞—Å—Ç)
        now = datetime.utcnow()
        ages = []
        for f in facts:
            age = (now - f.timestamp).total_seconds() / 3600  # —á–∞—Å—ã
            ages.append(age)
        avg_age = sum(ages) / len(ages) if ages else 0
        freshness = 1.0 if avg_age < 24 else 0.8 if avg_age < 168 else 0.5

        # Completeness
        completeness = min(1.0, len(facts) / 3)  # 3+ —Ñ–∞–∫—Ç–∞ = –ø–æ–ª–Ω—ã–π

        # Overall
        overall = (
            source_quality * 0.3
            + consensus * 0.3
            + freshness * 0.2
            + completeness * 0.2
        )

        # Gaps
        gaps = []
        if len(facts) < 2:
            gaps.append("–ú–∞–ª–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ (–Ω—É–∂–Ω–æ –º–∏–Ω–∏–º—É–º 2-3)")
        if contradictions:
            gaps.append(f"–ï—Å—Ç—å –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è ({len(contradictions)})")
        if source_quality < 0.5:
            gaps.append("–ù–∏–∑–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤")

        needs_more = overall < self.THRESHOLD_LOW or len(facts) < 2

        return ConfidenceAssessment(
            overall=round(overall, 2),
            source_quality=round(source_quality, 2),
            consensus=round(consensus, 2),
            freshness=round(freshness, 2),
            completeness=round(completeness, 2),
            needs_more_data=needs_more,
            gaps=gaps,
        )


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# STALENESS DETECTOR ‚Äî –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê


class StalenessDetector:
    """
    –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö.

    –ü—Ä–∞–≤–∏–ª–∞:
    - –ù–æ–≤–æ—Å—Ç–∏ > 7 –¥–Ω–µ–π ‚Äî —É—Å—Ç–∞—Ä–µ–≤—à–∏–µ
    - –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ > 1 –≥–æ–¥ ‚Äî –ø—Ä–æ–≤–µ—Ä–∏—Ç—å
    - –ù–∞—É—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ > 3 –ª–µ—Ç ‚Äî –ø—Ä–æ–≤–µ—Ä–∏—Ç—å
    - –ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ —Ñ–∞–∫—Ç—ã ‚Äî –Ω–µ —É—Å—Ç–∞—Ä–µ–≤–∞—é—Ç
    """

    # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏ –∏—Ö —Å—Ä–æ–∫–∏ –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç–∏ (–¥–Ω–∏)
    FRESHNESS_RULES: dict[str, int] = {
        "news": 7,
        "prices": 1,
        "weather": 1,
        "stocks": 1,
        "technology": 365,
        "science": 1095,
        "laws": 730,
        "general": 365,
        "history": 36500,  # 100 –ª–µ—Ç
    }

    def detect_category(self, query: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏—é –∑–∞–ø—Ä–æ—Å–∞."""
        lower = query.lower()

        patterns = {
            "news": ["–Ω–æ–≤–æ—Å—Ç", "—Å–µ–≥–æ–¥–Ω—è", "–≤—á–µ—Ä–∞", "news", "latest", "today"],
            "prices": ["—Ü–µ–Ω–∞", "—Å—Ç–æ–∏–º–æ—Å—Ç—å", "–∫—É—Ä—Å", "price", "cost", "rate"],
            "weather": ["–ø–æ–≥–æ–¥–∞", "weather", "forecast", "–ø—Ä–æ–≥–Ω–æ–∑"],
            "stocks": ["–∞–∫—Ü–∏–∏", "stock", "shares", "–±–∏—Ä–∂–∞", "market"],
            "technology": ["—Ç–µ—Ö–Ω–æ–ª–æ–≥", "software", "hardware", "tech", "ai"],
            "science": ["–Ω–∞—É–∫–∞", "research", "study", "–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω"],
            "laws": ["–∑–∞–∫–æ–Ω", "law", "regulation", "–ø—Ä–∞–≤–∏–ª–æ", "tax"],
            "history": ["–∏—Å—Ç–æ—Ä–∏", "history", "historical", "–≤ –ø—Ä–æ—à–ª–æ–º"],
        }

        for category, keywords in patterns.items():
            if any(k in lower for k in keywords):
                return category

        return "general"

    def is_stale(
        self,
        content_date: datetime,
        category: str = "general",
    ) -> tuple[bool, str]:
        """
        –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, —É—Å—Ç–∞—Ä–µ–ª–∏ –ª–∏ –¥–∞–Ω–Ω—ã–µ.

        Returns:
            (is_stale, reason)
        """
        max_age_days = self.FRESHNESS_RULES.get(category, 365)
        age = (datetime.utcnow() - content_date).days

        if age > max_age_days:
            return True, (
                f"–î–∞–Ω–Ω—ã–µ –æ—Ç {content_date.strftime('%d.%m.%Y')} "
                f"—É—Å—Ç–∞—Ä–µ–ª–∏ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ ¬´{category}¬ª "
                f"(–º–∞–∫—Å–∏–º—É–º {max_age_days} –¥–Ω–µ–π)"
            )

        return False, ""

    def filter_fresh(
        self,
        facts: list[FactClaim],
        category: str = "general",
    ) -> list[FactClaim]:
        """–û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ —Å–≤–µ–∂–∏–µ —Ñ–∞–∫—Ç—ã."""
        max_age_days = self.FRESHNESS_RULES.get(category, 365)
        cutoff = datetime.utcnow() - timedelta(days=max_age_days)

        return [f for f in facts if f.timestamp >= cutoff]


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# REASONING LAYER v2 ‚Äî –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê


@dataclass
class ReasoningResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç reasoning –ø—Ä–æ—Ü–µ—Å—Å–∞."""
    answer: str = ""
    confidence: ConfidenceAssessment = field(
        default_factory=ConfidenceAssessment)
    facts: list[FactClaim] = field(default_factory=list)
    contradictions: list[Contradiction] = field(default_factory=list)
    hypotheses: list[Hypothesis] = field(default_factory=list)
    sources_used: int = 0
    queries_expanded: int = 0
    reasoning_time_ms: int = 0
    compressed: bool = False

    def to_dict(self) -> dict:
        return {
            "answer": self.answer[:500],
            "confidence": self.confidence.to_dict(),
            "facts_count": len(self.facts),
            "contradictions_count": len(self.contradictions),
            "hypotheses": [h.to_dict() for h in self.hypotheses],
            "sources_used": self.sources_used,
            "queries_expanded": self.queries_expanded,
            "reasoning_time_ms": self.reasoning_time_ms,
        }


class ReasoningLayerV2:
    """
    –û–±—ä–µ–¥–∏–Ω—è—é—â–∏–π –¥–≤–∏–∂–æ–∫ reasoning.

    –ü—Ä–æ—Ü–µ—Å—Å:
    1. Expand query ‚Üí –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤
    2. Search ‚Üí —Å–æ–±—Ä–∞—Ç—å —Ñ–∞–∫—Ç—ã –∏–∑ 2-3+ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
    3. Score ‚Üí –æ—Ü–µ–Ω–∏—Ç—å trust –∫–∞–∂–¥–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
    4. Detect contradictions ‚Üí –Ω–∞–π—Ç–∏ —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏—è
    5. Check staleness ‚Üí –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å —É—Å—Ç–∞—Ä–µ–≤—à–µ–µ
    6. Assess confidence ‚Üí –Ω—É–∂–Ω–æ –ª–∏ –±–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö?
    7. If low confidence ‚Üí refine queries ‚Üí search again
    8. Compress ‚Üí —Å–∂–∞—Ç—å –¥–ª—è LLM
    9. Synthesize ‚Üí —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç
    """

    def __init__(self):
        self.trust_scorer = TrustScorerV2()
        self.contradiction_detector = ContradictionDetector()
        self.query_expander = QueryExpander()
        self.hypothesis_tester = HypothesisTester()
        self.compressor = ContextCompressor()
        self.confidence_engine = ConfidenceEngine()
        self.staleness_detector = StalenessDetector()

    def expand_query(self, query: str, max_queries: int = 3) -> list[str]:
        """–†–∞—Å—à–∏—Ä–∏—Ç—å –∑–∞–ø—Ä–æ—Å."""
        return self.query_expander.expand(query, max_queries)

    def score_facts(self, facts: list[FactClaim]) -> list[FactClaim]:
        """–û—Ü–µ–Ω–∏—Ç—å trust score –¥–ª—è –≤—Å–µ—Ö —Ñ–∞–∫—Ç–æ–≤."""
        for fact in facts:
            fact.trust_score = self.trust_scorer.score_content(
                fact.text, fact.source_url, fact.timestamp
            )
        return facts

    def find_contradictions(self, facts: list[FactClaim]) -> list[Contradiction]:
        """–ù–∞–π—Ç–∏ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è."""
        return self.contradiction_detector.detect(facts)

    def assess_confidence(
        self,
        facts: list[FactClaim],
        contradictions: list[Contradiction] | None = None,
        query: str = "",
    ) -> ConfidenceAssessment:
        """–û—Ü–µ–Ω–∏—Ç—å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å."""
        return self.confidence_engine.assess(facts, contradictions, query)

    def compress_context(self, text: str, max_length: int = 2000) -> str:
        """–°–∂–∞—Ç—å —Ç–µ–∫—Å—Ç."""
        return self.compressor.compress(text, max_length)

    def generate_hypotheses(self, question: str) -> list[Hypothesis]:
        """–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –≥–∏–ø–æ—Ç–µ–∑—ã."""
        return self.hypothesis_tester.generate_hypotheses(question)

    def test_hypothesis(
        self,
        hypothesis: Hypothesis,
        facts: list[FactClaim],
    ) -> Hypothesis:
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –≥–∏–ø–æ—Ç–µ–∑—É."""
        return self.hypothesis_tester.evaluate_hypothesis(hypothesis, facts)

    def get_stats(self) -> dict[str, Any]:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞."""
        return {
            "trust_domains_tracked": len(self.trust_scorer._history),
            "custom_scores": len(self.trust_scorer._custom_scores),
        }


# ‚îÄ‚îÄ‚îÄ –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

reasoning_v2 = ReasoningLayerV2()
