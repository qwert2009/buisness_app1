"""
PDS-Ultimate Advanced Memory Manager (Ñ‡Ğ°ÑÑ‚ÑŒ 2)
=================================================
ĞœĞµĞ½ĞµĞ´Ğ¶ĞµÑ€ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ¼, failure-driven learning,
time awareness, context compression, memory pruning.
"""

from __future__ import annotations

import json
import math
import re
from collections import Counter
from datetime import datetime, timedelta

from pds_ultimate.config import logger
from pds_ultimate.core.advanced_memory import (
    AdvancedMemoryEntry,
    AdvancedWorkingMemory,
    FailureEntry,
    MemoryType,
)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TF-IDF SEMANTIC SEARCH (Ğ±ĞµĞ· Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… embeddings)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class SemanticIndex:
    """
    Ğ¡ĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¸Ğ½Ğ´ĞµĞºÑ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¿Ğ¾ ÑĞ¼Ñ‹ÑĞ»Ñƒ.

    Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ TF-IDF + n-gram overlap + tag matching.
    Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ±ĞµĞ· Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… API (DeepSeek/OpenAI embeddings).
    ĞŸÑ€Ğ¸ Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸Ğ¸ embeddings API â€” Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ vector search.
    """

    # Ğ¡Ñ‚Ğ¾Ğ¿-ÑĞ»Ğ¾Ğ²Ğ° (Ñ€ÑƒÑÑĞºĞ¸Ğµ + Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¸Ğµ)
    STOP_WORDS = frozenset({
        "Ğ¸", "Ğ²", "Ğ½Ğ°", "Ñ", "Ğ¿Ğ¾", "Ğ´Ğ»Ñ", "Ğ¸Ğ·", "Ñ‡Ñ‚Ğ¾", "ÑÑ‚Ğ¾", "ĞºĞ°Ğº",
        "Ğ½Ğµ", "Ğ½Ğ¾", "Ğ¾Ñ‚", "Ğº", "Ğ·Ğ°", "Ñ‚Ğ¾", "Ğ¾Ğ½", "Ğ¾Ğ½Ğ°", "Ğ¼Ñ‹", "Ğ²Ñ‹",
        "a", "the", "is", "in", "on", "at", "to", "for", "of", "and",
        "or", "but", "it", "this", "that", "with", "from", "by", "be",
        "are", "was", "were", "been", "will", "would", "can", "could",
        "Ñ", "Ñ‚Ñ‹", "ĞµĞ³Ğ¾", "ĞµÑ‘", "Ğ¸Ñ…", "Ğ¼Ğ¾Ğ¹", "ÑĞ²Ğ¾Ğ¹", "Ğ²ÑĞµ", "Ñ‚Ğ°Ğº",
        "Ğ´Ğ°", "Ğ½ĞµÑ‚", "ÑƒĞ¶Ğµ", "ĞµÑ‰Ñ‘", "Ğ±Ñ‹", "Ğ»Ğ¸", "Ğ¶Ğµ", "ĞµÑĞ»Ğ¸", "ĞºĞ¾Ğ³Ğ´Ğ°",
    })

    def __init__(self):
        self._doc_freq: Counter = Counter()  # document frequency
        self._total_docs: int = 0

    def tokenize(self, text: str) -> list[str]:
        """Ğ¢Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹."""
        text = text.lower().strip()
        tokens = re.findall(r'[Ğ°-ÑÑ‘a-z0-9_-]{2,}', text)
        return [t for t in tokens if t not in self.STOP_WORDS]

    def bigrams(self, tokens: list[str]) -> list[str]:
        """Ğ‘Ğ¸Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñ‹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸."""
        return [f"{tokens[i]}_{tokens[i+1]}" for i in range(len(tokens) - 1)]

    def update_index(self, entries: list[AdvancedMemoryEntry]) -> None:
        """ĞĞ±Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒ Ğ¸Ğ½Ğ´ĞµĞºÑ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚ Ğ¸Ğ· Ğ²ÑĞµÑ… Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹."""
        self._doc_freq.clear()
        self._total_docs = len(entries)

        for entry in entries:
            tokens = set(self.tokenize(entry.content))
            tokens.update(t.lower() for t in entry.tags)
            for token in tokens:
                self._doc_freq[token] += 1

    def score(self, query: str, entry: AdvancedMemoryEntry) -> float:
        """
        Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ÑŒ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ·Ğ°Ğ¿Ğ¸ÑĞ¸ Ğº Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑƒ.

        Scoring = TF-IDF overlap + tag match + bigram match + type bonus
        """
        query_tokens = self.tokenize(query)
        if not query_tokens:
            return 0.0

        content_tokens = self.tokenize(entry.content)
        tag_tokens = [t.lower() for t in entry.tags]

        # TF-IDF scoring
        query_set = set(query_tokens)
        content_set = set(content_tokens)

        tfidf_score = 0.0
        for token in query_set & content_set:
            tf = content_tokens.count(token) / max(1, len(content_tokens))
            df = self._doc_freq.get(token, 1)
            idf = math.log(max(1, self._total_docs) / max(1, df))
            tfidf_score += tf * idf

        # Tag match (Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğ¹ Ğ²ĞµÑ)
        tag_set = set(tag_tokens)
        tag_overlap = len(query_set & tag_set)
        tag_score = tag_overlap * 2.0

        # Bigram match (Ñ„Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğµ ÑĞ¾Ğ²Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸Ğµ)
        query_bigrams = set(self.bigrams(query_tokens))
        content_bigrams = set(self.bigrams(content_tokens))
        bigram_overlap = len(query_bigrams & content_bigrams)
        bigram_score = bigram_overlap * 1.5

        # Effective importance
        eff_importance = entry.effective_importance()

        total = (tfidf_score + tag_score + bigram_score) * eff_importance
        return total


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CONTEXT COMPRESSOR
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class ContextCompressor:
    """
    Auto-summary & context compression.

    - Ğ ĞµĞºÑƒÑ€ÑĞ¸Ğ²Ğ½Ğ¾Ğµ ÑÑƒĞ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²
    - Chunking Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²
    - Compression ratio tracking
    """

    MAX_CONTEXT_CHARS = 4000
    CHUNK_SIZE = 2000

    @staticmethod
    def compress_history(
        history: list[dict[str, str]],
        max_messages: int = 10,
    ) -> list[dict[str, str]]:
        """
        Ğ¡Ğ¶Ğ°Ñ‚ÑŒ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°.

        Ğ¡Ñ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ:
        1. ĞŸĞ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğµ N ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹ â€” Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ
        2. Ğ‘Ğ¾Ğ»ĞµĞµ ÑÑ‚Ğ°Ñ€Ñ‹Ğµ â€” ÑĞ¶Ğ°Ñ‚ÑŒ Ğ² summary
        """
        if len(history) <= max_messages:
            return history

        # Ğ Ğ°Ğ·Ğ´ĞµĞ»ÑĞµĞ¼: ÑÑ‚Ğ°Ñ€Ñ‹Ğµ + Ğ½ĞµĞ´Ğ°Ğ²Ğ½Ğ¸Ğµ
        old = history[:-max_messages]
        recent = history[-max_messages:]

        # Ğ¡Ğ¶Ğ¸Ğ¼Ğ°ĞµĞ¼ ÑÑ‚Ğ°Ñ€Ñ‹Ğµ Ğ² 1 ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ
        summary_parts = []
        for msg in old:
            role = msg.get("role", "user")
            content = msg.get("content", "")[:100]
            summary_parts.append(f"[{role}] {content}")

        summary = (
            f"[Ğ¡Ğ¶Ğ°Ñ‚Ğ°Ñ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ ({len(old)} ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹)]\n"
            + "\n".join(summary_parts[-10:])  # ĞœĞ°ĞºÑ 10 ÑÑ‚Ñ€Ğ¾Ğº Ğ¸Ğ· ÑÑ‚Ğ°Ñ€Ñ‹Ñ…
        )

        compressed = [{"role": "system", "content": summary}] + recent
        return compressed

    @staticmethod
    def compress_text(text: str, max_length: int = 2000) -> str:
        """
        Ğ¡Ğ¶Ğ°Ñ‚ÑŒ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚, ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ğ² ĞºĞ»ÑÑ‡ĞµĞ²ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ.

        Ğ¡Ñ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ: Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ°Ğ±Ğ·Ğ°Ñ† + Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğ¹ Ğ°Ğ±Ğ·Ğ°Ñ† + ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ.
        """
        if len(text) <= max_length:
            return text

        paragraphs = text.split("\n\n")
        if len(paragraphs) <= 2:
            return text[:max_length] + "..."

        # ĞŸĞµÑ€Ğ²Ñ‹Ğ¹ + Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğ¹ Ğ°Ğ±Ğ·Ğ°Ñ†
        first = paragraphs[0][:max_length // 3]
        last = paragraphs[-1][:max_length // 3]

        # Ğ˜Ğ· ÑĞµÑ€ĞµĞ´Ğ¸Ğ½Ñ‹ â€” Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ñ Ñ‡Ğ¸ÑĞ»Ğ°Ğ¼Ğ¸ Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼Ğ¸ ÑĞ»Ğ¾Ğ²Ğ°Ğ¼Ğ¸
        middle = "\n\n".join(paragraphs[1:-1])
        key_sentences = []
        for sent in re.split(r'[.!?]\s+', middle):
            if any(c.isdigit() for c in sent) or len(sent.split()) > 5:
                key_sentences.append(sent.strip())
                if len("\n".join(key_sentences)) > max_length // 3:
                    break

        middle_text = ". ".join(key_sentences[:5])
        result = f"{first}\n\n[...ÑĞ¶Ğ°Ñ‚Ğ¾...]\n\n{middle_text}\n\n{last}"
        return result[:max_length]

    @staticmethod
    def chunk_text(text: str, chunk_size: int = 2000,
                   overlap: int = 200) -> list[str]:
        """Ğ Ğ°Ğ·Ğ±Ğ¸Ñ‚ÑŒ Ñ‚ĞµĞºÑÑ‚ Ğ½Ğ° chunks Ñ Ğ¿ĞµÑ€ĞµĞºÑ€Ñ‹Ñ‚Ğ¸ĞµĞ¼."""
        if len(text) <= chunk_size:
            return [text]

        chunks = []
        start = 0
        while start < len(text):
            end = start + chunk_size
            chunk = text[start:end]

            # Ğ˜Ñ‰ĞµĞ¼ ĞºĞ¾Ğ½ĞµÑ† Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ
            if end < len(text):
                last_period = chunk.rfind(".")
                if last_period > chunk_size // 2:
                    chunk = chunk[:last_period + 1]
                    end = start + last_period + 1

            chunks.append(chunk)
            start = end - overlap if end < len(text) else end

        return chunks


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ADVANCED MEMORY MANAGER
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class AdvancedMemoryManager:
    """
    ĞŸÑ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğ¹ Ğ¼ĞµĞ½ĞµĞ´Ğ¶ĞµÑ€ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸.

    Ğ’Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸:
    1. 5 Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ (episodic, semantic, procedural, strategic, failure)
    2. Failure-driven learning (Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº, Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ)
    3. Time awareness (decay, expiry, Ğ°ĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ)
    4. Auto-summary & context compression
    5. Memory pruning (ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ ÑƒÑÑ‚Ğ°Ñ€ĞµĞ²ÑˆĞ¸Ñ…)
    6. TF-IDF semantic search
    7. Per-user memory isolation
    8. Confidence tracking
    """

    MAX_MEMORIES = 2000
    PRUNE_THRESHOLD = 0.05  # Ğ£Ğ´Ğ°Ğ»ÑĞµĞ¼ Ğ·Ğ°Ğ¿Ğ¸ÑĞ¸ Ñ effective_importance < threshold
    PRUNE_AGE_DAYS = 90     # Ğ£Ğ´Ğ°Ğ»ÑĞµĞ¼ Ğ·Ğ°Ğ¿Ğ¸ÑĞ¸ ÑÑ‚Ğ°Ñ€ÑˆĞµ N Ğ´Ğ½ĞµĞ¹ Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ

    # ĞŸÑ€Ğ¾Ğ¼Ğ¿Ñ‚ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ñ„Ğ°ĞºÑ‚Ğ¾Ğ² (Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ‹Ğ¹)
    FACT_EXTRACTION_PROMPT = """ĞŸÑ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞ¹ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³ Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ¸ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğµ Ñ„Ğ°ĞºÑ‚Ñ‹.
ĞšĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞ¹ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ñ„Ğ°ĞºÑ‚ Ğ¿Ğ¾ Ñ‚Ğ¸Ğ¿Ñƒ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸.

Ğ’ĞµÑ€Ğ½Ğ¸ JSON Ğ¼Ğ°ÑÑĞ¸Ğ²:
[
  {{
    "fact": "ĞºÑ€Ğ°Ñ‚ĞºĞ¾Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ",
    "type": "episodic|semantic|procedural|strategic|preference|rule",
    "importance": 0.0-1.0,
    "confidence": 0.0-1.0,
    "tags": ["Ñ‚ĞµĞ³1", "Ñ‚ĞµĞ³2"],
    "expiry_days": null Ğ¸Ğ»Ğ¸ Ñ‡Ğ¸ÑĞ»Ğ¾ Ğ´Ğ½ĞµĞ¹ Ğ°ĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸
  }}
]

Ğ¢Ğ¸Ğ¿Ñ‹:
- episodic: ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğµ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğµ, Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ, Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚
- semantic: Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ñ‘Ğ½Ğ½Ğ¾Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğµ, Ñ„Ğ°ĞºÑ‚ Ğ¾ Ğ¼Ğ¸Ñ€Ğµ
- procedural: Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼, Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ğ°, ÑĞ¿Ğ¾ÑĞ¾Ğ± ÑĞ´ĞµĞ»Ğ°Ñ‚ÑŒ
- strategic: Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ¾ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚Ğ°Ñ…, Ğ¿Ğ»Ğ°Ğ½Ğ°Ñ…, ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸
- preference: Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ
- rule: Ğ±Ğ¸Ğ·Ğ½ĞµÑ-Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ¾

Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°Ğ¹ Ğ¢ĞĞ›Ğ¬ĞšĞ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğµ Ñ„Ğ°ĞºÑ‚Ñ‹.
ĞŸÑƒÑÑ‚Ğ¾Ğ¹ Ğ¼Ğ°ÑÑĞ¸Ğ² [] ĞµÑĞ»Ğ¸ Ğ½Ğ¸Ñ‡ĞµĞ³Ğ¾ Ğ²Ğ°Ğ¶Ğ½Ğ¾Ğ³Ğ¾ Ğ½ĞµÑ‚."""

    FAILURE_ANALYSIS_PROMPT = """ĞŸÑ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞ¹ Ğ½ĞµÑƒĞ´Ğ°Ñ‡Ñƒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°.

ĞÑˆĞ¸Ğ±ĞºĞ°: {error}
ĞšĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚: {context}
Ğ˜ÑÑ…Ğ¾Ğ´Ğ½Ğ°Ñ Ñ†ĞµĞ»ÑŒ: {goal}

Ğ’ĞµÑ€Ğ½Ğ¸ JSON:
{{
  "what_went_wrong": "ĞºÑ€Ğ°Ñ‚ĞºĞ¾Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸",
  "root_cause": "Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ°",
  "correction": "ĞºĞ°Ğº Ğ½Ğ°Ğ´Ğ¾ Ğ±Ñ‹Ğ»Ğ¾ ÑĞ´ĞµĞ»Ğ°Ñ‚ÑŒ",
  "severity": "low|medium|high|critical",
  "lesson": "ÑƒÑ€Ğ¾Ğº Ğ½Ğ° Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞµ",
  "tags": ["Ñ‚ĞµĞ³1", "Ñ‚ĞµĞ³2"]
}}"""

    def __init__(self):
        self._memories: list[AdvancedMemoryEntry] = []
        self._working: dict[int, AdvancedWorkingMemory] = {}
        self._index = SemanticIndex()
        self._compressor = ContextCompressor()
        self._index_dirty = True  # ĞÑƒĞ¶Ğ½Ğ¾ Ğ»Ğ¸ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒ Ğ¸Ğ½Ğ´ĞµĞºÑ

    # â”€â”€â”€ Working Memory â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def get_working(self, chat_id: int) -> AdvancedWorkingMemory:
        """ĞŸĞ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ¸Ğ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ±Ğ¾Ñ‡ÑƒÑ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ñ‡Ğ°Ñ‚Ğ°."""
        if chat_id not in self._working:
            self._working[chat_id] = AdvancedWorkingMemory()
        return self._working[chat_id]

    def reset_working(self, chat_id: int) -> None:
        """Ğ¡Ğ±Ñ€Ğ¾ÑĞ¸Ñ‚ÑŒ Ñ€Ğ°Ğ±Ğ¾Ñ‡ÑƒÑ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ."""
        if chat_id in self._working:
            self._working[chat_id].reset()

    # â”€â”€â”€ Store â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def store(self, entry: AdvancedMemoryEntry) -> None:
        """Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ğ·Ğ°Ğ¿Ğ¸ÑÑŒ Ñ Ğ´ĞµĞ´ÑƒĞ¿Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸ĞµĞ¹."""
        # Ğ”ĞµĞ´ÑƒĞ¿Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¿Ğ¾ context_hash
        for existing in self._memories:
            if existing.context_hash == entry.context_hash and existing.is_active:
                # ĞĞ±Ğ½Ğ¾Ğ²Ğ»ÑĞµĞ¼ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰ÑƒÑ
                existing.importance = max(
                    existing.importance, entry.importance)
                existing.confidence = max(
                    existing.confidence, entry.confidence)
                existing.touch()
                logger.debug(f"Memory deduplicated: {entry.content[:40]}...")
                return

        self._memories.append(entry)
        self._index_dirty = True
        self._enforce_limits()
        logger.debug(
            f"Memory stored: [{entry.memory_type}] {entry.content[:50]}..."
        )

    def store_fact(self, content: str, importance: float = 0.5,
                   confidence: float = 0.8, tags: list[str] | None = None,
                   source: str = "extraction",
                   chat_id: int | None = None) -> AdvancedMemoryEntry:
        """Ğ‘Ñ‹ÑÑ‚Ñ€Ğ¾Ğµ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ñ„Ğ°ĞºÑ‚Ğ°."""
        entry = AdvancedMemoryEntry(
            content=content,
            memory_type=MemoryType.FACT,
            importance=importance,
            confidence=confidence,
            tags=tags or [],
            source=source,
            chat_id=chat_id,
        )
        self.store(entry)
        return entry

    def store_preference(self, content: str, importance: float = 0.7,
                         chat_id: int | None = None) -> AdvancedMemoryEntry:
        """Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğµ."""
        entry = AdvancedMemoryEntry(
            content=content,
            memory_type=MemoryType.PREFERENCE,
            importance=importance,
            confidence=0.9,
            tags=["preference", "user"],
            source="extraction",
            decay_rate=0.02,  # ĞŸÑ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ Ğ½Ğµ Ğ·Ğ°Ğ±Ñ‹Ğ²Ğ°ÑÑ‚ÑÑ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾
            chat_id=chat_id,
        )
        self.store(entry)
        return entry

    def store_rule(self, content: str, importance: float = 0.8,
                   chat_id: int | None = None) -> AdvancedMemoryEntry:
        """Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ğ±Ğ¸Ğ·Ğ½ĞµÑ-Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ¾."""
        entry = AdvancedMemoryEntry(
            content=content,
            memory_type=MemoryType.RULE,
            importance=importance,
            confidence=0.9,
            tags=["rule", "business"],
            source="extraction",
            decay_rate=0.01,  # ĞŸÑ€Ğ°Ğ²Ğ¸Ğ»Ğ° Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸ Ğ½Ğµ Ğ·Ğ°Ğ±Ñ‹Ğ²Ğ°ÑÑ‚ÑÑ
            chat_id=chat_id,
        )
        self.store(entry)
        return entry

    def store_procedural(self, content: str, importance: float = 0.7,
                         tags: list[str] | None = None,
                         chat_id: int | None = None) -> AdvancedMemoryEntry:
        """Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ğ½Ğ¾Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğµ (ĞºĞ°Ğº Ğ´ĞµĞ»Ğ°Ñ‚ÑŒ)."""
        entry = AdvancedMemoryEntry(
            content=content,
            memory_type=MemoryType.PROCEDURAL,
            importance=importance,
            confidence=0.8,
            tags=(tags or []) + ["procedural", "how-to"],
            source="extraction",
            decay_rate=0.03,
            chat_id=chat_id,
        )
        self.store(entry)
        return entry

    def store_strategic(self, content: str, importance: float = 0.9,
                        tags: list[str] | None = None,
                        chat_id: int | None = None) -> AdvancedMemoryEntry:
        """Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ."""
        entry = AdvancedMemoryEntry(
            content=content,
            memory_type=MemoryType.STRATEGIC,
            importance=importance,
            confidence=0.85,
            tags=(tags or []) + ["strategic", "decision"],
            source="extraction",
            decay_rate=0.005,  # Ğ¡Ñ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¶Ğ¸Ğ²ÑƒÑ‚ Ğ´Ğ¾Ğ»Ğ³Ğ¾
            chat_id=chat_id,
        )
        self.store(entry)
        return entry

    # â”€â”€â”€ Failure-Driven Learning â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def store_failure(
        self,
        content: str,
        error_context: str = "",
        correction: str = "",
        severity: str = "medium",
        tags: list[str] | None = None,
        chat_id: int | None = None,
    ) -> FailureEntry:
        """
        Failure-driven learning: ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ğ¾ÑˆĞ¸Ğ±ĞºÑƒ.

        ĞĞ³ĞµĞ½Ñ‚ ÑƒÑ‡Ğ¸Ñ‚ÑÑ Ğ½Ğ° Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ñ… â€” Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶ĞµĞ¹ ÑĞ¸Ñ‚ÑƒĞ°Ñ†Ğ¸Ğ¸
        ĞĞ• Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€ÑĞµÑ‚ Ğ½ĞµÑƒĞ´Ğ°Ñ‡Ğ½Ğ¾Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ.
        """
        entry = FailureEntry(
            content=content,
            error_context=error_context,
            correction=correction,
            severity=severity,
            tags=tags or [],
            source="failure_learning",
            chat_id=chat_id,
        )
        self.store(entry)
        logger.info(
            f"Failure stored [{severity}]: {content[:60]}..."
        )
        return entry

    def get_relevant_failures(
        self, query: str, limit: int = 3
    ) -> list[FailureEntry]:
        """ĞĞ°Ğ¹Ñ‚Ğ¸ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ (Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ½Ğµ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€ÑÑ‚ÑŒ)."""
        failures = [
            m for m in self._memories
            if m.memory_type == MemoryType.FAILURE
            and m.is_active
            and not m.is_expired()
        ]

        if not failures:
            return []

        self._rebuild_index_if_needed()
        scored = []
        for f in failures:
            score = self._index.score(query, f)
            if score > 0:
                scored.append((score, f))

        scored.sort(key=lambda x: x[0], reverse=True)
        return [f for _, f in scored[:limit]]

    # â”€â”€â”€ Semantic Recall â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def recall(
        self,
        query: str,
        limit: int = 5,
        memory_type: str | None = None,
        tags: list[str] | None = None,
        min_importance: float = 0.0,
        chat_id: int | None = None,
    ) -> list[AdvancedMemoryEntry]:
        """
        Semantic recall â€” Ğ¿Ğ¾Ğ¸ÑĞº Ğ¿Ğ¾ ÑĞ¼Ñ‹ÑĞ»Ñƒ (TF-IDF + tags + decay).
        """
        self._rebuild_index_if_needed()

        candidates = [
            m for m in self._memories
            if m.is_active and not m.is_expired()
        ]

        # Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€Ñ‹
        if memory_type:
            candidates = [
                m for m in candidates if m.memory_type == memory_type]
        if tags:
            candidates = [
                m for m in candidates if any(t in m.tags for t in tags)
            ]
        if min_importance > 0:
            candidates = [
                m for m in candidates
                if m.effective_importance() >= min_importance
            ]
        if chat_id is not None:
            candidates = [
                m for m in candidates
                if m.chat_id is None or m.chat_id == chat_id
            ]

        # TF-IDF scoring
        scored = []
        for m in candidates:
            score = self._index.score(query, m)
            if score > 0:
                scored.append((score, m))

        scored.sort(key=lambda x: x[0], reverse=True)

        results = []
        for _, m in scored[:limit]:
            m.touch()
            results.append(m)

        return results

    def recall_all(
        self,
        memory_type: str | None = None,
        min_importance: float = 0.0,
        limit: int = 20,
        chat_id: int | None = None,
    ) -> list[AdvancedMemoryEntry]:
        """ĞŸĞ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ²ÑĞµ Ğ²Ğ¾ÑĞ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ñ, Ğ¾Ñ‚ÑĞ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾ effective importance."""
        candidates = [
            m for m in self._memories
            if m.is_active and not m.is_expired()
        ]
        if memory_type:
            candidates = [
                m for m in candidates if m.memory_type == memory_type]
        if min_importance > 0:
            candidates = [
                m for m in candidates
                if m.effective_importance() >= min_importance
            ]
        if chat_id is not None:
            candidates = [
                m for m in candidates
                if m.chat_id is None or m.chat_id == chat_id
            ]

        candidates.sort(key=lambda m: m.effective_importance(), reverse=True)
        return candidates[:limit]

    def get_context_for_prompt(
        self, query: str, max_entries: int = 7,
        chat_id: int | None = None,
    ) -> str:
        """
        ĞŸĞ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ¸Ğ· Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ´Ğ»Ñ system prompt.

        Ğ¡Ğ¾Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ¸Ğ· Ğ’Ğ¡Ğ•Ğ¥ Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ â€” Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ„Ğ°ĞºÑ‚Ñ‹.
        Ğ’ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ failures (ÑƒÑ€Ğ¾ĞºĞ¸), procedures (ĞºĞ°Ğº Ğ´ĞµĞ»Ğ°Ñ‚ÑŒ), strategies.
        """
        results: list[AdvancedMemoryEntry] = []

        # Ğ˜Ğ· ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ñ‚Ğ¸Ğ¿Ğ° Ğ±ĞµÑ€Ñ‘Ğ¼ ÑĞ°Ğ¼Ğ¾Ğµ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾Ğµ
        for mem_type in [
            MemoryType.FACT, MemoryType.PREFERENCE, MemoryType.RULE,
            MemoryType.STRATEGIC, MemoryType.PROCEDURAL,
        ]:
            found = self.recall(
                query, limit=2, memory_type=mem_type, chat_id=chat_id
            )
            results.extend(found)

        # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ ÑƒÑ€Ğ¾ĞºĞ¸ Ğ¸Ğ· Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº
        failures = self.get_relevant_failures(query, limit=2)
        results.extend(failures)

        # Ğ”ĞµĞ´ÑƒĞ¿Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ñ
        seen = set()
        unique = []
        for e in results:
            if e.content not in seen:
                seen.add(e.content)
                unique.append(e)

        if not unique:
            return ""

        # Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¿Ğ¾ effective importance
        unique.sort(key=lambda e: e.effective_importance(), reverse=True)
        unique = unique[:max_entries]

        lines = ["Ğ”ĞĞ›Ğ“ĞĞ¡Ğ ĞĞ§ĞĞĞ¯ ĞŸĞĞœĞ¯Ğ¢Ğ¬:"]
        for e in unique:
            icon = {
                MemoryType.FACT: "ğŸ“Œ",
                MemoryType.PREFERENCE: "â­",
                MemoryType.RULE: "ğŸ“",
                MemoryType.SEMANTIC: "ğŸ“š",
                MemoryType.PROCEDURAL: "ğŸ”§",
                MemoryType.STRATEGIC: "ğŸ¯",
                MemoryType.FAILURE: "âš ï¸",
                MemoryType.EPISODIC: "ğŸ“–",
            }.get(e.memory_type, "â€¢")

            conf = f"[conf={e.confidence:.0%}]" if e.confidence < 0.7 else ""
            line = f"  {icon} {e.content}"
            if conf:
                line += f" {conf}"

            # Ğ”Ğ»Ñ failures â€” Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ correction
            if isinstance(e, FailureEntry) and e.correction:
                line += f"\n     â†’ Ğ£Ñ€Ğ¾Ğº: {e.correction}"

            lines.append(line)

        return "\n".join(lines)

    # â”€â”€â”€ Time Awareness â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def get_time_context(self) -> str:
        """
        Time awareness â€” Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ñ‚ĞµĞºÑƒÑ‰ĞµĞ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°.
        ĞĞ³ĞµĞ½Ñ‚ Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Â«ĞºĞ¾Ğ³Ğ´Ğ° ÑĞµĞ¹Ñ‡Ğ°ÑÂ» Ğ¸ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ°ĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ.
        """
        now = datetime.utcnow()
        return (
            f"Ğ¢Ğ•ĞšĞ£Ğ©Ğ•Ğ• Ğ’Ğ Ğ•ĞœĞ¯: {now.strftime('%Y-%m-%d %H:%M')} UTC\n"
            f"Ğ”ĞµĞ½ÑŒ Ğ½ĞµĞ´ĞµĞ»Ğ¸: {now.strftime('%A')}\n"
            f"âš ï¸ ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞ¹ Ğ°ĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. "
            f"Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ ÑÑ‚Ğ°Ñ€ÑˆĞµ 2024 Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ±Ñ‹Ñ‚ÑŒ ÑƒÑÑ‚Ğ°Ñ€ĞµĞ²ÑˆĞ¸Ğ¼Ğ¸."
        )

    # â”€â”€â”€ Memory Pruning â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def prune(self) -> int:
        """
        Memory embedding pruning â€” ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ ÑƒÑÑ‚Ğ°Ñ€ĞµĞ²ÑˆĞ¸Ñ… Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹.

        Ğ£Ğ´Ğ°Ğ»ÑĞµÑ‚:
        1. Expired Ğ·Ğ°Ğ¿Ğ¸ÑĞ¸
        2. Ğ—Ğ°Ğ¿Ğ¸ÑĞ¸ Ñ effective_importance < threshold
        3. Ğ¡Ñ‚Ğ°Ñ€Ñ‹Ğµ Ğ·Ğ°Ğ¿Ğ¸ÑĞ¸ Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ 0 Ğ¾Ğ±Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğ¹
        """
        before = len(self._memories)
        now = datetime.utcnow()
        cutoff = now - timedelta(days=self.PRUNE_AGE_DAYS)

        active = []
        for m in self._memories:
            # Ğ£Ğ´Ğ°Ğ»ÑĞµĞ¼ expired
            if m.is_expired():
                continue

            # Ğ£Ğ´Ğ°Ğ»ÑĞµĞ¼ Ñ Ğ¾Ñ‡ĞµĞ½ÑŒ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ
            if m.effective_importance() < self.PRUNE_THRESHOLD:
                continue

            # Ğ£Ğ´Ğ°Ğ»ÑĞµĞ¼ ÑÑ‚Ğ°Ñ€Ñ‹Ğµ Ğ½ĞµĞ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ñ‹Ğµ
            if (m.created_at < cutoff
                    and m.access_count == 0
                    and m.importance < 0.3):
                continue

            active.append(m)

        self._memories = active
        pruned = before - len(self._memories)

        if pruned > 0:
            self._index_dirty = True
            logger.info(f"Memory pruned: {pruned} Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¾")

        return pruned

    # â”€â”€â”€ Persist to/from DB â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def save_to_db(self, db_session) -> int:
        """Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ğ²ÑĞµ unsaved memories Ğ² Ğ‘Ğ”."""
        from pds_ultimate.core.database import AgentMemory

        count = 0
        for m in self._memories:
            if m.db_id is not None:
                continue  # Ğ£Ğ¶Ğµ Ğ² Ğ‘Ğ”

            metadata = m.metadata.copy()
            metadata["confidence"] = m.confidence
            metadata["decay_rate"] = m.decay_rate
            metadata["source_quality"] = m.source_quality
            metadata["failure_count"] = m.failure_count
            metadata["success_count"] = m.success_count
            metadata["context_hash"] = m.context_hash
            if m.chat_id is not None:
                metadata["chat_id"] = m.chat_id
            if m.expiry:
                metadata["expiry"] = m.expiry.isoformat()
            if isinstance(m, FailureEntry):
                metadata["error_context"] = m.error_context
                metadata["correction"] = m.correction
                metadata["severity"] = m.severity

            db_entry = AgentMemory(
                content=m.content,
                memory_type=m.memory_type,
                importance=m.importance,
                tags=json.dumps(m.tags, ensure_ascii=False),
                source=m.source,
                metadata_json=json.dumps(
                    metadata, ensure_ascii=False, default=str
                ),
                access_count=m.access_count,
            )
            db_session.add(db_entry)
            db_session.flush()
            m.db_id = db_entry.id
            count += 1

        if count > 0:
            db_session.commit()
            logger.info(f"Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¾ {count} Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ² Ğ‘Ğ”")
        return count

    def load_from_db(self, db_session) -> int:
        """Ğ—Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚ÑŒ memories Ğ¸Ğ· Ğ‘Ğ”."""
        from pds_ultimate.core.database import AgentMemory

        try:
            db_entries = db_session.query(AgentMemory).filter_by(
                is_active=True
            ).order_by(
                AgentMemory.importance.desc()
            ).limit(self.MAX_MEMORIES).all()

            count = 0
            existing_ids = {
                m.db_id for m in self._memories if m.db_id is not None
            }

            for db_entry in db_entries:
                if db_entry.id in existing_ids:
                    continue

                tags = []
                try:
                    tags = json.loads(db_entry.tags) if db_entry.tags else []
                except (json.JSONDecodeError, TypeError):
                    pass

                metadata = {}
                try:
                    metadata = json.loads(
                        db_entry.metadata_json
                    ) if db_entry.metadata_json else {}
                except (json.JSONDecodeError, TypeError):
                    pass

                # Ğ’Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµĞ¼ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ»Ñ Ğ¸Ğ· metadata
                confidence = float(metadata.get("confidence", 0.8))
                decay_rate = float(metadata.get("decay_rate", 0.1))
                source_quality = float(metadata.get("source_quality", 0.7))
                chat_id = metadata.get("chat_id")
                expiry = None
                if metadata.get("expiry"):
                    try:
                        expiry = datetime.fromisoformat(metadata["expiry"])
                    except (ValueError, TypeError):
                        pass

                # FailureEntry Ğ¸Ğ»Ğ¸ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ°Ñ
                if db_entry.memory_type == MemoryType.FAILURE:
                    entry = FailureEntry(
                        content=db_entry.content,
                        error_context=metadata.get("error_context", ""),
                        correction=metadata.get("correction", ""),
                        severity=metadata.get("severity", "medium"),
                        importance=db_entry.importance,
                        confidence=confidence,
                        tags=tags,
                        source=db_entry.source or "db",
                        decay_rate=decay_rate,
                        source_quality=source_quality,
                        chat_id=chat_id,
                    )
                else:
                    entry = AdvancedMemoryEntry(
                        content=db_entry.content,
                        memory_type=db_entry.memory_type,
                        importance=db_entry.importance,
                        confidence=confidence,
                        tags=tags,
                        source=db_entry.source or "db",
                        metadata=metadata,
                        decay_rate=decay_rate,
                        expiry=expiry,
                        source_quality=source_quality,
                        chat_id=chat_id,
                    )

                entry.db_id = db_entry.id
                entry.access_count = db_entry.access_count or 0
                entry.created_at = db_entry.created_at
                entry.failure_count = int(metadata.get("failure_count", 0))
                entry.success_count = int(metadata.get("success_count", 0))

                self._memories.append(entry)
                count += 1

            if count > 0:
                self._index_dirty = True
            logger.info(f"Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ¾ {count} Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸Ğ· Ğ‘Ğ”")
            return count
        except Exception as e:
            logger.warning(f"ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚ÑŒ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ¸Ğ· Ğ‘Ğ”: {e}")
            return 0

    # â”€â”€â”€ Fact Extraction â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    async def extract_and_store_facts(
        self, dialogue: str, llm_engine=None,
        chat_id: int | None = None,
    ) -> list[AdvancedMemoryEntry]:
        """Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ÑŒ Ñ„Ğ°ĞºÑ‚Ñ‹ Ğ¸Ğ· Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ° Ñ‡ĞµÑ€ĞµĞ· LLM."""
        if not llm_engine:
            from pds_ultimate.core.llm_engine import llm_engine as default_engine
            llm_engine = default_engine

        try:
            response = await llm_engine.chat(
                message=f"Ğ”Ğ¸Ğ°Ğ»Ğ¾Ğ³:\n{dialogue}",
                system_prompt=self.FACT_EXTRACTION_PROMPT,
                task_type="parse_order",
                temperature=0.2,
                json_mode=True,
            )

            facts_data = json.loads(response)
            if not isinstance(facts_data, list):
                return []

            stored = []
            for fact_data in facts_data:
                if not isinstance(fact_data, dict):
                    continue

                content = fact_data.get("fact", "").strip()
                if not content:
                    continue

                # ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµĞ¼ expiry
                expiry = None
                expiry_days = fact_data.get("expiry_days")
                if expiry_days and isinstance(expiry_days, (int, float)):
                    expiry = datetime.utcnow() + timedelta(days=expiry_days)

                entry = AdvancedMemoryEntry(
                    content=content,
                    memory_type=fact_data.get("type", MemoryType.FACT),
                    importance=float(fact_data.get("importance", 0.5)),
                    confidence=float(fact_data.get("confidence", 0.8)),
                    tags=fact_data.get("tags", []),
                    source="extraction",
                    expiry=expiry,
                    chat_id=chat_id,
                )
                self.store(entry)
                stored.append(entry)

            if stored:
                logger.info(f"Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¾ {len(stored)} Ñ„Ğ°ĞºÑ‚Ğ¾Ğ² Ğ¸Ğ· Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°")
            return stored

        except Exception as e:
            logger.warning(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ñ„Ğ°ĞºÑ‚Ğ¾Ğ²: {e}")
            return []

    # â”€â”€â”€ Failure Analysis â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    async def analyze_and_store_failure(
        self,
        error: str,
        context: str,
        goal: str,
        llm_engine=None,
        chat_id: int | None = None,
    ) -> FailureEntry | None:
        """
        Failure-driven learning: Ğ¿Ñ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾ÑˆĞ¸Ğ±ĞºÑƒ Ñ‡ĞµÑ€ĞµĞ· LLM.

        ĞĞ³ĞµĞ½Ñ‚ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾ÑˆĞ»Ğ¾ Ğ½Ğµ Ñ‚Ğ°Ğº Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ ÑƒÑ€Ğ¾Ğº.
        Ğ’ Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞ¼ Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶ĞµĞ¹ ÑĞ¸Ñ‚ÑƒĞ°Ñ†Ğ¸Ğ¸ Ğ½Ğµ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ¸Ñ‚ Ğ¾ÑˆĞ¸Ğ±ĞºÑƒ.
        """
        if not llm_engine:
            from pds_ultimate.core.llm_engine import llm_engine as default_engine
            llm_engine = default_engine

        prompt = self.FAILURE_ANALYSIS_PROMPT.format(
            error=error, context=context, goal=goal,
        )

        try:
            response = await llm_engine.chat(
                message=prompt,
                system_prompt="Ğ¢Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸Ğº Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº. ĞÑ‚Ğ²ĞµÑ‡Ğ°Ğ¹ JSON.",
                task_type="parse_order",
                temperature=0.2,
                json_mode=True,
            )

            analysis = json.loads(response)
            if not isinstance(analysis, dict):
                return None

            failure = self.store_failure(
                content=analysis.get("what_went_wrong", error),
                error_context=analysis.get("root_cause", context),
                correction=analysis.get("correction", ""),
                severity=analysis.get("severity", "medium"),
                tags=analysis.get("tags", []) + ["auto_analyzed"],
                chat_id=chat_id,
            )

            # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ ÑƒÑ€Ğ¾Ğº ĞºĞ°Ğº Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğµ
            lesson = analysis.get("lesson", "")
            if lesson:
                self.store_fact(
                    content=f"Ğ£Ğ ĞĞš: {lesson}",
                    importance=0.8,
                    confidence=0.85,
                    tags=["lesson", "failure_learning"],
                    source="failure_analysis",
                    chat_id=chat_id,
                )

            return failure

        except Exception as e:
            logger.warning(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° failure: {e}")
            # Fallback: ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ±ĞµĞ· LLM-Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°
            return self.store_failure(
                content=error[:200],
                error_context=context[:200],
                correction="",
                severity="medium",
                chat_id=chat_id,
            )

    # â”€â”€â”€ History Consolidation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    async def consolidate_history(
        self,
        history: list[dict[str, str]],
        llm_engine=None,
    ) -> str:
        """Ğ¡Ğ¶Ğ°Ñ‚ÑŒ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ + Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ÑŒ Ñ„Ğ°ĞºÑ‚Ñ‹."""
        if not llm_engine:
            from pds_ultimate.core.llm_engine import llm_engine as default_engine
            llm_engine = default_engine

        dialogue = "\n".join(
            f"{msg['role']}: {msg['content']}" for msg in history
        )

        try:
            await self.extract_and_store_facts(dialogue, llm_engine)

            consolidation_prompt = (
                "Ğ¡Ğ¾Ğ¶Ğ¼Ğ¸ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³ Ğ² ĞºÑ€Ğ°Ñ‚ĞºĞ¾Ğµ ÑĞ°Ğ¼Ğ¼Ğ°Ñ€Ğ¸.\n"
                "Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚:\nĞ¡ĞĞœĞœĞĞ Ğ˜: [2-3 Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ]\n"
                "Ğ¤ĞĞšĞ¢Ğ«: [ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ñ‡ĞµÑ€ĞµĞ· |]\n"
                "Ğ Ğ•Ğ¨Ğ•ĞĞ˜Ğ¯: [Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· |]"
            )

            summary = await llm_engine.chat(
                message=dialogue,
                system_prompt=consolidation_prompt,
                task_type="summarize",
                temperature=0.3,
            )
            return summary
        except Exception as e:
            logger.warning(f"ĞÑˆĞ¸Ğ±ĞºĞ° ÑĞ¶Ğ°Ñ‚Ğ¸Ñ: {e}")
            return f"[Ğ˜ÑÑ‚Ğ¾Ñ€Ğ¸Ñ Ğ¸Ğ· {len(history)} ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹]"

    # â”€â”€â”€ Internal â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _rebuild_index_if_needed(self) -> None:
        """ĞŸĞµÑ€ĞµÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ÑŒ Ğ¸Ğ½Ğ´ĞµĞºÑ ĞµÑĞ»Ğ¸ Ğ½ÑƒĞ¶Ğ½Ğ¾."""
        if self._index_dirty:
            active = [m for m in self._memories if m.is_active]
            self._index.update_index(active)
            self._index_dirty = False

    def _enforce_limits(self) -> None:
        """Ğ£Ğ´Ğ°Ğ»Ğ¸Ñ‚ÑŒ Ğ½Ğ°Ğ¸Ğ¼ĞµĞ½ĞµĞµ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğµ ĞµÑĞ»Ğ¸ Ğ»Ğ¸Ğ¼Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ²Ñ‹ÑˆĞµĞ½."""
        if len(self._memories) <= self.MAX_MEMORIES:
            return

        # Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¿Ğ¾ effective importance
        self._memories.sort(key=lambda m: m.effective_importance())
        excess = len(self._memories) - self.MAX_MEMORIES
        removed = self._memories[:excess]
        self._memories = self._memories[excess:]

        for r in removed:
            r.is_active = False

        self._index_dirty = True
        logger.debug(f"Memory limit: ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¾ {len(removed)} Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹")

    # â”€â”€â”€ Stats â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    @property
    def total_count(self) -> int:
        """ĞĞ±Ñ‰ĞµĞµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹."""
        return sum(1 for m in self._memories if m.is_active)

    def get_stats(self) -> dict:
        """Ğ Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ° Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸."""
        active = [m for m in self._memories if m.is_active]
        type_counts: dict[str, int] = {}
        total_confidence = 0.0
        total_effective = 0.0
        failures_count = 0

        for m in active:
            type_counts[m.memory_type] = type_counts.get(m.memory_type, 0) + 1
            total_confidence += m.confidence
            total_effective += m.effective_importance()
            if m.memory_type == MemoryType.FAILURE:
                failures_count += 1

        n = max(1, len(active))
        return {
            "total": len(active),
            "total_with_inactive": len(self._memories),
            "by_type": type_counts,
            "avg_importance": sum(m.importance for m in active) / n,
            "avg_confidence": total_confidence / n,
            "avg_effective_importance": total_effective / n,
            "failures_stored": failures_count,
            "working_memories": len(self._working),
            "index_dirty": self._index_dirty,
        }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# BACKWARD COMPATIBILITY WRAPPER
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# ĞĞ»Ğ¸Ğ°ÑÑ‹ Ğ´Ğ»Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ ÑĞ¾ ÑÑ‚Ğ°Ñ€Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼
MemoryEntry = AdvancedMemoryEntry
WorkingMemory = AdvancedWorkingMemory
MemoryManager = AdvancedMemoryManager

# â”€â”€â”€ Ğ“Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞºĞ·ĞµĞ¼Ğ¿Ğ»ÑÑ€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

advanced_memory_manager = AdvancedMemoryManager()
