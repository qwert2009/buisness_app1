"""
PDS-Ultimate Browser Engine (Part 4)
========================================
ĞŸĞ¾Ğ»Ğ½Ğ¾Ñ†ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ±Ñ€Ğ°ÑƒĞ·ĞµÑ€Ğ½Ñ‹Ğ¹ Ğ´Ğ²Ğ¸Ğ¶Ğ¾Ğº Ğ½Ğ° Playwright.

Ğ’Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸:
1. Anti-Detection â€” stealth mode, Ñ€Ğ°Ğ½Ğ´Ğ¾Ğ¼Ğ½Ñ‹Ğµ fingerprints
2. Human-Like Behavior â€” ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¸, Ğ¿Ğ»Ğ°Ğ²Ğ½Ñ‹Ğ¹ ÑĞºÑ€Ğ¾Ğ»Ğ», Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¼Ñ‹ÑˆĞ¸
3. Page Management â€” Ğ¿ÑƒĞ» ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†, Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ñ, Ğ¾Ğ¶Ğ¸Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸
4. Data Extraction â€” Ñ‚ĞµĞºÑÑ‚, HTML, Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹, ÑÑÑ‹Ğ»ĞºĞ¸, Ğ¼ĞµÑ‚Ğ°-Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ
5. Screenshots â€” Ğ¿Ğ¾Ğ»Ğ½Ğ°Ñ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ğ°, ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚, Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑŒ
6. Form Automation â€” Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ñ„Ğ¾Ñ€Ğ¼, ĞºĞ»Ğ¸ĞºĞ¸, Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ğ¸Ğ· ÑĞ¿Ğ¸ÑĞºĞ¾Ğ²
7. Cookie/Session Management â€” ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ/Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° ÑĞµÑÑĞ¸Ğ¹
8. Download Management â€” ÑĞºĞ°Ñ‡Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ²
9. Web Search â€” Ğ¿Ğ¾Ğ¸ÑĞº Ñ‡ĞµÑ€ĞµĞ· DuckDuckGo (Ğ±ĞµĞ· API ĞºĞ»ÑÑ‡ĞµĞ¹)
10. Tool Integration â€” browser tools Ğ´Ğ»Ñ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ°

Anti-Detection Features:
- Ğ Ğ°Ğ½Ğ´Ğ¾Ğ¼Ğ½Ñ‹Ğµ User-Agent Ğ¸Ğ· Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿ÑƒĞ»Ğ°
- Ğ¡ĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ navigator.webdriver
- ĞŸĞ¾Ğ´Ğ¼ĞµĞ½Ğ° canvas/WebGL fingerprint
- Ğ ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ viewport Ğ¸ screen Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ñ‹
- Human-like timing (ĞºĞ»Ğ¸ĞºĞ¸, Ğ½Ğ°Ğ±Ğ¾Ñ€ Ñ‚ĞµĞºÑÑ‚Ğ°)
- Ğ Ğ°Ğ½Ğ´Ğ¾Ğ¼Ğ½Ğ¾Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¼Ñ‹ÑˆĞ¸ Ğ¿ĞµÑ€ĞµĞ´ ĞºĞ»Ğ¸ĞºĞ¾Ğ¼
- Ğ ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ ÑĞºÑ€Ğ¾Ğ»Ğ»Ğ¸Ğ½Ğ³
"""

from __future__ import annotations

import asyncio
import hashlib
import json
import random
import re
import time
from dataclasses import dataclass, field
from enum import Enum
from pathlib import Path
from typing import Any
from urllib.parse import quote_plus

from pds_ultimate.config import config, logger

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 1. CONSTANTS & USER AGENTS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Ğ ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ Ğ¿ÑƒĞ» User-Agent'Ğ¾Ğ² (Ğ°ĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ½Ğ° 2026)
USER_AGENTS = [
    # Chrome Windows
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
    "(KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
    "(KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36",
    # Chrome macOS
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 "
    "(KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
    # Chrome Linux
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 "
    "(KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
    # Firefox Windows
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:134.0) "
    "Gecko/20100101 Firefox/134.0",
    # Firefox macOS
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:134.0) "
    "Gecko/20100101 Firefox/134.0",
    # Firefox Linux
    "Mozilla/5.0 (X11; Linux x86_64; rv:134.0) "
    "Gecko/20100101 Firefox/134.0",
    # Edge
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
    "(KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36 Edg/131.0.0.0",
]

# Ğ ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑĞºÑ€Ğ°Ğ½Ğ°
SCREEN_SIZES = [
    (1920, 1080), (1366, 768), (1536, 864), (1440, 900),
    (1280, 720), (2560, 1440), (1600, 900), (1680, 1050),
]

# Stealth JS â€” ÑĞºÑ€Ğ¸Ğ¿Ñ‚ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´Ğ¼ĞµĞ½Ñ‹ fingerprint
STEALTH_JS = """
// Hide webdriver
Object.defineProperty(navigator, 'webdriver', {get: () => undefined});

// Hide automation
delete navigator.__proto__.webdriver;

// Chrome runtime
window.chrome = {
    runtime: {
        onConnect: undefined,
        onMessage: undefined,
    },
    loadTimes: function() { return {}; },
    csi: function() { return {}; },
};

// Permissions
const originalQuery = window.navigator.permissions.query;
window.navigator.permissions.query = (parameters) =>
    parameters.name === 'notifications'
        ? Promise.resolve({state: Notification.permission})
        : originalQuery(parameters);

// Plugins â€” simulate realistic
Object.defineProperty(navigator, 'plugins', {
    get: () => {
        const plugins = [
            {name: 'Chrome PDF Plugin', filename: 'internal-pdf-viewer'},
            {name: 'Chrome PDF Viewer', filename: 'mhjfbmdgcfjbbpaeojofohoefgiehjai'},
            {name: 'Native Client', filename: 'internal-nacl-plugin'},
        ];
        plugins.length = 3;
        return plugins;
    },
});

// Languages
Object.defineProperty(navigator, 'languages', {
    get: () => ['en-US', 'en', 'ru'],
});

// Platform
Object.defineProperty(navigator, 'platform', {
    get: () => 'Win32',
});

// Hardware concurrency
Object.defineProperty(navigator, 'hardwareConcurrency', {
    get: () => 8,
});

// Device memory
Object.defineProperty(navigator, 'deviceMemory', {
    get: () => 8,
});

// WebGL vendor
const getParameter = WebGLRenderingContext.prototype.getParameter;
WebGLRenderingContext.prototype.getParameter = function(parameter) {
    if (parameter === 37445) return 'Intel Inc.';
    if (parameter === 37446) return 'Intel Iris OpenGL Engine';
    return getParameter.call(this, parameter);
};
"""


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 2. DATA MODELS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class PageStatus(str, Enum):
    """Ğ¡Ñ‚Ğ°Ñ‚ÑƒÑ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹."""
    LOADING = "loading"
    READY = "ready"
    ERROR = "error"
    CLOSED = "closed"


@dataclass
class PageInfo:
    """Ğ˜Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ğµ."""
    url: str
    title: str = ""
    status_code: int = 200
    status: PageStatus = PageStatus.READY
    load_time_ms: int = 0
    content_type: str = ""
    text_length: int = 0


@dataclass
class ExtractedData:
    """Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡Ñ‘Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞ¾ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹."""
    url: str
    title: str = ""
    text: str = ""
    links: list[dict[str, str]] = field(default_factory=list)
    images: list[dict[str, str]] = field(default_factory=list)
    tables: list[list[list[str]]] = field(default_factory=list)
    meta: dict[str, str] = field(default_factory=dict)
    headings: list[dict[str, str]] = field(default_factory=list)
    forms: list[dict] = field(default_factory=list)

    def summary(self, max_text: int = 2000) -> str:
        """ĞšÑ€Ğ°Ñ‚ĞºĞ¾Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡Ñ‘Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."""
        parts = [f"ğŸ“„ {self.title}", f"ğŸ”— {self.url}"]
        if self.text:
            trimmed = self.text[:max_text]
            if len(self.text) > max_text:
                trimmed += "..."
            parts.append(f"\n{trimmed}")
        if self.links:
            parts.append(f"\nğŸ”— Ğ¡ÑÑ‹Ğ»Ğ¾Ğº: {len(self.links)}")
        if self.tables:
            parts.append(f"ğŸ“Š Ğ¢Ğ°Ğ±Ğ»Ğ¸Ñ†: {len(self.tables)}")
        if self.images:
            parts.append(f"ğŸ–¼ï¸ Ğ˜Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹: {len(self.images)}")
        return "\n".join(parts)

    def to_dict(self) -> dict:
        """Ğ¡ĞµÑ€Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ."""
        return {
            "url": self.url,
            "title": self.title,
            "text_length": len(self.text),
            "links_count": len(self.links),
            "tables_count": len(self.tables),
            "images_count": len(self.images),
            "meta": self.meta,
        }


@dataclass
class SearchResult:
    """Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ Ğ¿Ğ¾Ğ¸ÑĞºĞ°."""
    title: str
    url: str
    snippet: str = ""
    position: int = 0

    def __str__(self) -> str:
        return f"[{self.position}] {self.title}\n    {self.url}\n    {self.snippet}"


@dataclass
class BrowserStats:
    """Ğ¡Ñ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ±Ñ€Ğ°ÑƒĞ·ĞµÑ€Ğ°."""
    pages_loaded: int = 0
    pages_failed: int = 0
    screenshots_taken: int = 0
    searches_performed: int = 0
    total_bytes_downloaded: int = 0
    total_time_ms: int = 0
    errors: list[str] = field(default_factory=list)

    def to_dict(self) -> dict:
        return {
            "pages_loaded": self.pages_loaded,
            "pages_failed": self.pages_failed,
            "screenshots": self.screenshots_taken,
            "searches": self.searches_performed,
            "bytes_downloaded": self.total_bytes_downloaded,
            "time_ms": self.total_time_ms,
            "errors_count": len(self.errors),
        }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 3. HUMAN-LIKE BEHAVIOR
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class HumanBehavior:
    """
    Ğ˜Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ² Ğ±Ñ€Ğ°ÑƒĞ·ĞµÑ€Ğµ.

    - Ğ¡Ğ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑĞ¼Ğ¸
    - ĞŸĞ»Ğ°Ğ²Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ñ‚ĞµĞºÑÑ‚Ğ° Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ğ¼Ğ¸ Ğ¸ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸
    - Ğ ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ ÑĞºÑ€Ğ¾Ğ»Ğ»Ğ¸Ğ½Ğ³
    - Ğ”Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¼Ñ‹ÑˆĞ¸ Ğº ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ñƒ Ğ¿ĞµÑ€ĞµĞ´ ĞºĞ»Ğ¸ĞºĞ¾Ğ¼
    - Ğ¡Ğ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ñ‹Ğµ Ğ¿Ğ°ÑƒĞ·Ñ‹ (ĞºĞ°Ğº Ğ±ÑƒĞ´Ñ‚Ğ¾ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞº Ğ´ÑƒĞ¼Ğ°ĞµÑ‚)
    """

    def __init__(
        self,
        min_type_delay: int = 50,
        max_type_delay: int = 150,
        min_click_delay: int = 100,
        max_click_delay: int = 500,
    ):
        self.min_type_delay = min_type_delay
        self.max_type_delay = max_type_delay
        self.min_click_delay = min_click_delay
        self.max_click_delay = max_click_delay

    async def random_delay(self, min_ms: int = 100, max_ms: int = 500) -> None:
        """Ğ¡Ğ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ°Ñ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ°."""
        delay = random.randint(min_ms, max_ms) / 1000
        await asyncio.sleep(delay)

    async def thinking_pause(self) -> None:
        """ĞŸĞ°ÑƒĞ·Ğ° 'Ğ½Ğ° Ğ¿Ğ¾Ğ´ÑƒĞ¼Ğ°Ñ‚ÑŒ' (0.5-2 ÑĞµĞº)."""
        await asyncio.sleep(random.uniform(0.5, 2.0))

    async def human_type(self, page: Any, selector: str, text: str) -> None:
        """
        ĞŸĞµÑ‡Ğ°Ñ‚ÑŒ Ñ‚ĞµĞºÑÑ‚Ğ° Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒÑ.
        Ğ’ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ°Ğ¼Ğ¸.
        """
        element = page.locator(selector)
        await element.click()
        await self.random_delay(200, 500)

        for char in text:
            await element.press_sequentially(
                char,
                delay=random.randint(self.min_type_delay, self.max_type_delay),
            )
            # Ğ˜Ğ½Ğ¾Ğ³Ğ´Ğ° Ğ´ĞµĞ»Ğ°ĞµĞ¼ Ğ¼Ğ¸Ğ½Ğ¸-Ğ¿Ğ°ÑƒĞ·Ñƒ (ĞºĞ°Ğº Ğ±ÑƒĞ´Ñ‚Ğ¾ Ğ´ÑƒĞ¼Ğ°ĞµĞ¼)
            if random.random() < 0.05:
                await asyncio.sleep(random.uniform(0.3, 0.8))

    async def human_click(self, page: Any, selector: str) -> None:
        """
        ĞšĞ»Ğ¸Ğº Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹.
        ĞŸĞµÑ€ĞµĞ´ ĞºĞ»Ğ¸ĞºĞ¾Ğ¼ â€” Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğµ Ğ¾Ğ¶Ğ¸Ğ´Ğ°Ğ½Ğ¸Ğµ.
        """
        await self.random_delay(self.min_click_delay, self.max_click_delay)
        element = page.locator(selector)

        # Ğ¡ĞºÑ€Ğ¾Ğ»Ğ»Ğ¸Ğ¼ Ğº ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ñƒ
        await element.scroll_into_view_if_needed()
        await self.random_delay(100, 300)

        # ĞšĞ»Ğ¸ĞºĞ°ĞµĞ¼
        await element.click()

    async def smooth_scroll(self, page: Any, direction: str = "down",
                            distance: int = 300) -> None:
        """ĞŸĞ»Ğ°Ğ²Ğ½Ñ‹Ğ¹ ÑĞºÑ€Ğ¾Ğ»Ğ»Ğ¸Ğ½Ğ³."""
        if direction == "down":
            delta = distance
        elif direction == "up":
            delta = -distance
        else:
            delta = distance

        # ĞĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ¼Ğ°Ğ»ĞµĞ½ÑŒĞºĞ¸Ñ… ÑĞºÑ€Ğ¾Ğ»Ğ»Ğ¾Ğ² Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ³Ğ¾
        steps = random.randint(3, 6)
        step_size = delta // steps

        for _ in range(steps):
            await page.mouse.wheel(0, step_size + random.randint(-20, 20))
            await asyncio.sleep(random.uniform(0.05, 0.15))

    async def scroll_to_bottom(self, page: Any, max_scrolls: int = 10) -> None:
        """ĞŸÑ€Ğ¾ĞºÑ€ÑƒÑ‚ĞºĞ° Ğ´Ğ¾ ĞºĞ¾Ğ½Ñ†Ğ° ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹."""
        for _ in range(max_scrolls):
            prev_height = await page.evaluate("document.body.scrollHeight")
            await self.smooth_scroll(page, "down", random.randint(400, 800))
            await asyncio.sleep(random.uniform(0.5, 1.5))
            new_height = await page.evaluate("document.body.scrollHeight")
            if new_height == prev_height:
                break

    def get_random_user_agent(self) -> str:
        """Ğ¡Ğ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ñ‹Ğ¹ User-Agent Ğ¸Ğ· Ğ¿ÑƒĞ»Ğ°."""
        return random.choice(USER_AGENTS)

    def get_random_screen_size(self) -> tuple[int, int]:
        """Ğ¡Ğ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ ÑĞºÑ€Ğ°Ğ½Ğ°."""
        return random.choice(SCREEN_SIZES)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 4. BROWSER ENGINE â€” Main Class
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class BrowserEngine:
    """
    Ğ‘Ñ€Ğ°ÑƒĞ·ĞµÑ€Ğ½Ñ‹Ğ¹ Ğ´Ğ²Ğ¸Ğ¶Ğ¾Ğº Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ĞºĞ»Ğ°ÑÑĞ°.

    Playwright + Anti-Detection + Human-Like + Data Extraction.

    Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ:
        engine = BrowserEngine()
        await engine.start()

        # ĞÑ‚ĞºÑ€Ñ‹Ñ‚ÑŒ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñƒ
        page_info = await engine.goto("https://example.com")

        # Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ÑŒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ
        data = await engine.extract_data()

        # ĞŸĞ¾Ğ¸ÑĞº Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚Ğµ
        results = await engine.web_search("python playwright tutorial")

        # Ğ¡ĞºÑ€Ğ¸Ğ½ÑˆĞ¾Ñ‚
        path = await engine.screenshot()

        await engine.stop()
    """

    def __init__(self, cfg: Any | None = None):
        self._cfg = cfg or config.browser
        self._human = HumanBehavior(
            min_type_delay=self._cfg.min_type_delay,
            max_type_delay=self._cfg.max_type_delay,
            min_click_delay=self._cfg.min_click_delay,
            max_click_delay=self._cfg.max_click_delay,
        )
        self._playwright = None
        self._browser = None
        self._context = None
        self._page = None
        self._pages: dict[str, Any] = {}  # id â†’ page
        self._stats = BrowserStats()
        self._started = False
        self._user_agent = (
            self._cfg.user_agent or self._human.get_random_user_agent()
        )

    @property
    def is_started(self) -> bool:
        return self._started

    @property
    def stats(self) -> BrowserStats:
        return self._stats

    @property
    def current_url(self) -> str:
        if self._page:
            return self._page.url
        return ""

    @property
    def current_title(self) -> str:
        """Ğ—Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğº Ñ‚ĞµĞºÑƒÑ‰ĞµĞ¹ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ (ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğ¹, ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹)."""
        return self._last_title

    # â”€â”€â”€ Lifecycle â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    async def start(self) -> None:
        """Ğ—Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚ÑŒ Ğ±Ñ€Ğ°ÑƒĞ·ĞµÑ€."""
        if self._started:
            return

        try:
            from playwright.async_api import async_playwright
        except ImportError:
            logger.error(
                "Playwright Ğ½Ğµ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½. Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚Ğµ: pip install playwright && "
                "playwright install chromium"
            )
            raise RuntimeError("Playwright not installed")

        self._playwright = await async_playwright().start()

        # Ğ’Ñ‹Ğ±Ğ¾Ñ€ Ñ‚Ğ¸Ğ¿Ğ° Ğ±Ñ€Ğ°ÑƒĞ·ĞµÑ€Ğ°
        browser_type = self._cfg.browser_type
        launcher = getattr(self._playwright, browser_type,
                           self._playwright.chromium)

        # ĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ·Ğ°Ğ¿ÑƒÑĞºĞ°
        launch_args = [
            "--disable-blink-features=AutomationControlled",
            "--disable-infobars",
            "--no-first-run",
            "--no-default-browser-check",
            "--disable-extensions",
        ]

        launch_kwargs: dict[str, Any] = {
            "headless": self._cfg.headless,
            "args": launch_args,
        }

        if self._cfg.proxy_server:
            launch_kwargs["proxy"] = {"server": self._cfg.proxy_server}

        self._browser = await launcher.launch(**launch_kwargs)

        # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‘Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ñ anti-detection
        width, height = self._cfg.viewport_width, self._cfg.viewport_height
        context_kwargs: dict[str, Any] = {
            "viewport": {"width": width, "height": height},
            "screen": {"width": width, "height": height},
            "user_agent": self._user_agent,
            "locale": self._cfg.locale,
            "timezone_id": self._cfg.timezone,
            "java_script_enabled": True,
            "accept_downloads": True,
            "ignore_https_errors": True,
        }

        self._context = await self._browser.new_context(**context_kwargs)

        # Stealth: Ğ¸Ğ½ÑŠĞµĞºÑ†Ğ¸Ñ JS Ğ´Ğ»Ñ ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸
        if self._cfg.stealth_enabled:
            await self._context.add_init_script(STEALTH_JS)

        # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‘Ğ¼ Ğ¿ĞµÑ€Ğ²ÑƒÑ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñƒ
        self._page = await self._context.new_page()
        self._page.set_default_timeout(self._cfg.default_timeout)
        self._page.set_default_navigation_timeout(self._cfg.navigation_timeout)
        self._last_title = ""

        # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‘Ğ¼ Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸
        self._cfg.screenshots_dir.mkdir(parents=True, exist_ok=True)
        self._cfg.downloads_dir.mkdir(parents=True, exist_ok=True)

        self._started = True
        logger.info(
            f"Browser Engine Ğ·Ğ°Ğ¿ÑƒÑ‰ĞµĞ½: {browser_type} "
            f"(headless={self._cfg.headless}, stealth={self._cfg.stealth_enabled})"
        )

    async def stop(self) -> None:
        """ĞÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒ Ğ±Ñ€Ğ°ÑƒĞ·ĞµÑ€."""
        if not self._started:
            return

        try:
            # Ğ—Ğ°ĞºÑ€Ñ‹Ğ²Ğ°ĞµĞ¼ Ğ²ÑĞµ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹
            for page in list(self._pages.values()):
                try:
                    await page.close()
                except Exception:
                    pass

            if self._page:
                try:
                    await self._page.close()
                except Exception:
                    pass

            if self._context:
                await self._context.close()
            if self._browser:
                await self._browser.close()
            if self._playwright:
                await self._playwright.stop()
        except Exception as e:
            logger.warning(f"Browser stop error: {e}")
        finally:
            self._started = False
            self._playwright = None
            self._browser = None
            self._context = None
            self._page = None
            self._pages.clear()
            logger.info("Browser Engine Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½")

    async def restart(self) -> None:
        """ĞŸĞµÑ€ĞµĞ·Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚ÑŒ Ğ±Ñ€Ğ°ÑƒĞ·ĞµÑ€."""
        await self.stop()
        # ĞĞ¾Ğ²Ñ‹Ğ¹ User-Agent Ğ¿Ñ€Ğ¸ Ğ¿ĞµÑ€ĞµĞ·Ğ°Ğ¿ÑƒÑĞºĞµ
        self._user_agent = self._human.get_random_user_agent()
        await self.start()

    # â”€â”€â”€ Navigation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    async def goto(self, url: str, wait_until: str = "domcontentloaded") -> PageInfo:
        """
        ĞŸĞµÑ€ĞµĞ¹Ñ‚Ğ¸ Ğ½Ğ° URL.

        Args:
            url: URL ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹
            wait_until: ĞšĞ¾Ğ³Ğ´Ğ° ÑÑ‡Ğ¸Ñ‚Ğ°Ñ‚ÑŒ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºÑƒ Ğ·Ğ°Ğ²ĞµÑ€ÑˆÑ‘Ğ½Ğ½Ğ¾Ğ¹
                       (domcontentloaded | load | networkidle | commit)

        Returns:
            PageInfo Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¾ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ğµ
        """
        if not self._started:
            await self.start()

        start_time = time.time()
        page_info = PageInfo(url=url)

        try:
            response = await self._page.goto(url, wait_until=wait_until)

            load_ms = int((time.time() - start_time) * 1000)
            page_info.load_time_ms = load_ms
            page_info.title = await self._page.title()
            self._last_title = page_info.title

            if response:
                page_info.status_code = response.status
                page_info.content_type = response.headers.get(
                    "content-type", "")

            page_info.status = PageStatus.READY
            self._stats.pages_loaded += 1
            self._stats.total_time_ms += load_ms

            logger.debug(
                f"Page loaded: {url} ({page_info.status_code}, {load_ms}ms)"
            )

        except Exception as e:
            page_info.status = PageStatus.ERROR
            page_info.status_code = 0
            self._stats.pages_failed += 1
            error_msg = f"Navigation error: {url} â€” {e}"
            self._stats.errors.append(error_msg)
            logger.warning(error_msg)

        return page_info

    async def go_back(self) -> PageInfo:
        """ĞĞ°Ğ·Ğ°Ğ´ Ğ² Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸."""
        if not self._page:
            return PageInfo(url="", status=PageStatus.ERROR)
        start = time.time()
        await self._page.go_back()
        title = await self._page.title()
        self._last_title = title
        return PageInfo(
            url=self._page.url,
            title=title,
            load_time_ms=int((time.time() - start) * 1000),
        )

    async def go_forward(self) -> PageInfo:
        """Ğ’Ğ¿ĞµÑ€Ñ‘Ğ´ Ğ² Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸."""
        if not self._page:
            return PageInfo(url="", status=PageStatus.ERROR)
        start = time.time()
        await self._page.go_forward()
        title = await self._page.title()
        self._last_title = title
        return PageInfo(
            url=self._page.url,
            title=title,
            load_time_ms=int((time.time() - start) * 1000),
        )

    async def reload(self) -> PageInfo:
        """ĞŸĞµÑ€ĞµĞ·Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚ÑŒ Ñ‚ĞµĞºÑƒÑ‰ÑƒÑ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñƒ."""
        if not self._page:
            return PageInfo(url="", status=PageStatus.ERROR)
        return await self.goto(self._page.url)

    # â”€â”€â”€ Data Extraction â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    async def extract_data(self, url: str | None = None) -> ExtractedData:
        """
        Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ÑŒ Ğ²ÑĞµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞ¾ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹.

        Args:
            url: URL (ĞµÑĞ»Ğ¸ Ğ½Ğµ Ğ·Ğ°Ğ´Ğ°Ğ½ â€” Ñ‚ĞµĞºÑƒÑ‰Ğ°Ñ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ğ°)

        Returns:
            ExtractedData ÑĞ¾ Ğ²ÑĞµĞ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸
        """
        if url:
            await self.goto(url)

        if not self._page:
            return ExtractedData(url="")

        current_url = self._page.url
        title = await self._page.title()

        data = ExtractedData(url=current_url, title=title)

        # Ğ¢ĞµĞºÑÑ‚ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹
        try:
            data.text = await self._page.evaluate("""
                () => {
                    const body = document.body;
                    if (!body) return '';
                    // Ğ£Ğ´Ğ°Ğ»ÑĞµĞ¼ script, style, nav, footer
                    const clone = body.cloneNode(true);
                    const remove = clone.querySelectorAll(
                        'script, style, nav, footer, header, aside, ' +
                        'noscript, iframe, svg, [aria-hidden="true"]'
                    );
                    remove.forEach(el => el.remove());
                    return clone.innerText || clone.textContent || '';
                }
            """)
            # ĞÑ‡Ğ¸Ñ‰Ğ°ĞµĞ¼ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ñ‹/Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑÑ‹
            data.text = re.sub(r'\n{3,}', '\n\n', data.text).strip()
            data.text = re.sub(r'[ \t]{2,}', ' ', data.text)
        except Exception as e:
            logger.debug(f"Text extraction error: {e}")

        # Ğ¡ÑÑ‹Ğ»ĞºĞ¸
        try:
            data.links = await self._page.evaluate("""
                () => {
                    const links = [];
                    document.querySelectorAll('a[href]').forEach(a => {
                        const href = a.href;
                        const text = (a.innerText || a.textContent || '').trim();
                        if (href && text && !href.startsWith('javascript:')) {
                            links.push({url: href, text: text.substring(0, 200)});
                        }
                    });
                    return links.slice(0, 100);
                }
            """)
        except Exception as e:
            logger.debug(f"Links extraction error: {e}")

        # Ğ—Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞ¸
        try:
            data.headings = await self._page.evaluate("""
                () => {
                    const headings = [];
                    document.querySelectorAll('h1, h2, h3, h4, h5, h6').forEach(h => {
                        const text = (h.innerText || h.textContent || '').trim();
                        if (text) {
                            headings.push({level: h.tagName.toLowerCase(), text: text});
                        }
                    });
                    return headings;
                }
            """)
        except Exception as e:
            logger.debug(f"Headings extraction error: {e}")

        # Ğ˜Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ
        try:
            data.images = await self._page.evaluate("""
                () => {
                    const images = [];
                    document.querySelectorAll('img[src]').forEach(img => {
                        const src = img.src;
                        const alt = img.alt || '';
                        if (src) {
                            images.push({src: src, alt: alt});
                        }
                    });
                    return images.slice(0, 50);
                }
            """)
        except Exception as e:
            logger.debug(f"Images extraction error: {e}")

        # Ğ¢Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹
        try:
            data.tables = await self._page.evaluate("""
                () => {
                    const tables = [];
                    document.querySelectorAll('table').forEach(table => {
                        const rows = [];
                        table.querySelectorAll('tr').forEach(tr => {
                            const cells = [];
                            tr.querySelectorAll('td, th').forEach(cell => {
                                cells.push((cell.innerText || cell.textContent || '').trim());
                            });
                            if (cells.length > 0) rows.push(cells);
                        });
                        if (rows.length > 0) tables.push(rows);
                    });
                    return tables.slice(0, 10);
                }
            """)
        except Exception as e:
            logger.debug(f"Tables extraction error: {e}")

        # Meta tags
        try:
            data.meta = await self._page.evaluate("""
                () => {
                    const meta = {};
                    document.querySelectorAll('meta[name], meta[property]').forEach(m => {
                        const key = m.getAttribute('name') || m.getAttribute('property');
                        const content = m.getAttribute('content');
                        if (key && content) meta[key] = content;
                    });
                    return meta;
                }
            """)
        except Exception as e:
            logger.debug(f"Meta extraction error: {e}")

        return data

    async def extract_text(self, url: str | None = None,
                           selector: str | None = None) -> str:
        """
        Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ÑŒ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‚ĞµĞºÑÑ‚ ÑĞ¾ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹.

        Args:
            url: URL (Ğ¾Ğ¿Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾)
            selector: CSS-ÑĞµĞ»ĞµĞºÑ‚Ğ¾Ñ€ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ°

        Returns:
            Ğ¢ĞµĞºÑÑ‚ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹
        """
        if url:
            await self.goto(url)

        if not self._page:
            return ""

        try:
            if selector:
                element = self._page.locator(selector).first
                return await element.inner_text()
            else:
                data = await self.extract_data()
                return data.text
        except Exception as e:
            logger.debug(f"Text extraction error: {e}")
            return ""

    async def extract_links(self, url: str | None = None,
                            pattern: str | None = None) -> list[dict[str, str]]:
        """
        Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ÑŒ ÑÑÑ‹Ğ»ĞºĞ¸ ÑĞ¾ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹.

        Args:
            url: URL (Ğ¾Ğ¿Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾)
            pattern: Regex Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€ Ğ´Ğ»Ñ URL

        Returns:
            Ğ¡Ğ¿Ğ¸ÑĞ¾Ğº ÑÑÑ‹Ğ»Ğ¾Ğº [{url, text}, ...]
        """
        if url:
            await self.goto(url)

        data = await self.extract_data()
        links = data.links

        if pattern:
            regex = re.compile(pattern, re.IGNORECASE)
            links = [l for l in links if regex.search(l.get("url", ""))]

        return links

    # â”€â”€â”€ Screenshots â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    async def screenshot(
        self,
        path: str | Path | None = None,
        full_page: bool = False,
        selector: str | None = None,
    ) -> Path:
        """
        Ğ¡Ğ´ĞµĞ»Ğ°Ñ‚ÑŒ ÑĞºÑ€Ğ¸Ğ½ÑˆĞ¾Ñ‚.

        Args:
            path: ĞŸÑƒÑ‚ÑŒ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ (Ğ¸Ğ»Ğ¸ auto-generated)
            full_page: ĞŸĞ¾Ğ»Ğ½Ğ°Ñ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ğ°
            selector: CSS-ÑĞµĞ»ĞµĞºÑ‚Ğ¾Ñ€ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ°

        Returns:
            Path Ğº ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ñ‘Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ñ„Ğ°Ğ¹Ğ»Ñƒ
        """
        if not self._page:
            raise RuntimeError("Browser not started")

        if not path:
            ts = int(time.time())
            url_hash = hashlib.md5(self._page.url.encode()).hexdigest()[:8]
            filename = f"screenshot_{ts}_{url_hash}.png"
            path = self._cfg.screenshots_dir / filename

        path = Path(path)
        path.parent.mkdir(parents=True, exist_ok=True)

        if selector:
            element = self._page.locator(selector).first
            await element.screenshot(path=str(path))
        else:
            await self._page.screenshot(path=str(path), full_page=full_page)

        self._stats.screenshots_taken += 1
        logger.debug(f"Screenshot saved: {path}")
        return path

    # â”€â”€â”€ Form Interaction â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    async def fill(self, selector: str, value: str,
                   human_like: bool = True) -> None:
        """
        Ğ—Ğ°Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ÑŒ Ğ¿Ğ¾Ğ»Ğµ Ñ„Ğ¾Ñ€Ğ¼Ñ‹.

        Args:
            selector: CSS-ÑĞµĞ»ĞµĞºÑ‚Ğ¾Ñ€
            value: Ğ—Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ
            human_like: ĞŸĞµÑ‡Ğ°Ñ‚Ğ°Ñ‚ÑŒ ĞºĞ°Ğº Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞº
        """
        if not self._page:
            raise RuntimeError("Browser not started")

        if human_like:
            await self._human.human_type(self._page, selector, value)
        else:
            await self._page.fill(selector, value)

    async def click(self, selector: str, human_like: bool = True) -> None:
        """ĞšĞ»Ğ¸ĞºĞ½ÑƒÑ‚ÑŒ Ğ¿Ğ¾ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ñƒ."""
        if not self._page:
            raise RuntimeError("Browser not started")

        if human_like:
            await self._human.human_click(self._page, selector)
        else:
            await self._page.click(selector)

    async def select_option(self, selector: str, value: str) -> None:
        """Ğ’Ñ‹Ğ±Ñ€Ğ°Ñ‚ÑŒ Ğ¾Ğ¿Ñ†Ğ¸Ñ Ğ² select."""
        if not self._page:
            raise RuntimeError("Browser not started")
        await self._page.select_option(selector, value)

    async def check(self, selector: str) -> None:
        """ĞÑ‚Ğ¼ĞµÑ‚Ğ¸Ñ‚ÑŒ Ñ‡ĞµĞºĞ±Ğ¾ĞºÑ."""
        if not self._page:
            raise RuntimeError("Browser not started")
        await self._page.check(selector)

    async def uncheck(self, selector: str) -> None:
        """Ğ¡Ğ½ÑÑ‚ÑŒ Ñ‡ĞµĞºĞ±Ğ¾ĞºÑ."""
        if not self._page:
            raise RuntimeError("Browser not started")
        await self._page.uncheck(selector)

    # â”€â”€â”€ Wait & Query â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    async def wait_for(self, selector: str, state: str = "visible",
                       timeout: int | None = None) -> bool:
        """
        ĞĞ¶Ğ¸Ğ´Ğ°Ñ‚ÑŒ Ğ¿Ğ¾ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ°.

        Args:
            selector: CSS-ÑĞµĞ»ĞµĞºÑ‚Ğ¾Ñ€
            state: visible | hidden | attached | detached
            timeout: Ğ¢Ğ°Ğ¹Ğ¼Ğ°ÑƒÑ‚ Ğ² Ğ¼Ñ

        Returns:
            True ĞµÑĞ»Ğ¸ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½
        """
        if not self._page:
            return False

        try:
            await self._page.locator(selector).wait_for(
                state=state, timeout=timeout or self._cfg.default_timeout
            )
            return True
        except Exception:
            return False

    async def query_selector(self, selector: str) -> bool:
        """ĞŸÑ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸Ğµ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ°."""
        if not self._page:
            return False
        try:
            count = await self._page.locator(selector).count()
            return count > 0
        except Exception:
            return False

    async def get_attribute(self, selector: str, attribute: str) -> str | None:
        """ĞŸĞ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ°."""
        if not self._page:
            return None
        try:
            return await self._page.locator(selector).first.get_attribute(attribute)
        except Exception:
            return None

    async def evaluate(self, expression: str) -> Any:
        """Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ÑŒ JavaScript Ğ½Ğ° ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ğµ."""
        if not self._page:
            return None
        try:
            return await self._page.evaluate(expression)
        except Exception as e:
            logger.debug(f"JS evaluate error: {e}")
            return None

    # â”€â”€â”€ Web Search â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    async def web_search(self, query: str, max_results: int = 10) -> list[SearchResult]:
        """
        ĞŸĞ¾Ğ¸ÑĞº Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚Ğµ Ñ‡ĞµÑ€ĞµĞ· DuckDuckGo HTML.

        ĞĞµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ API ĞºĞ»ÑÑ‡ĞµĞ¹. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ HTML-Ğ²ĞµÑ€ÑĞ¸Ñ DDG.

        Args:
            query: ĞŸĞ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ğ¹ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ
            max_results: ĞœĞ°ĞºÑĞ¸Ğ¼ÑƒĞ¼ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²

        Returns:
            Ğ¡Ğ¿Ğ¸ÑĞ¾Ğº SearchResult
        """
        if not self._started:
            await self.start()

        results: list[SearchResult] = []
        search_url = f"https://html.duckduckgo.com/html/?q={quote_plus(query)}"

        try:
            await self.goto(search_url)
            await self._human.random_delay(500, 1500)

            # Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµĞ¼ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ DuckDuckGo HTML
            raw_results = await self._page.evaluate("""
                () => {
                    const results = [];
                    document.querySelectorAll('.result').forEach(r => {
                        const titleEl = r.querySelector('.result__title a, .result__a');
                        const snippetEl = r.querySelector('.result__snippet');
                        const urlEl = r.querySelector('.result__url');
                        if (titleEl) {
                            results.push({
                                title: (titleEl.innerText || '').trim(),
                                url: titleEl.href || '',
                                snippet: snippetEl
                                    ? (snippetEl.innerText || '').trim()
                                    : '',
                            });
                        }
                    });
                    return results;
                }
            """)

            for i, item in enumerate(raw_results[:max_results]):
                url = item.get("url", "")
                # DDG redirect URL â€” Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµĞ¼ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹
                if "duckduckgo.com" in url and "uddg=" in url:
                    try:
                        from urllib.parse import parse_qs
                        from urllib.parse import urlparse as _urlparse
                        parsed = _urlparse(url)
                        real_url = parse_qs(parsed.query).get("uddg", [url])[0]
                        url = real_url
                    except Exception:
                        pass

                results.append(SearchResult(
                    title=item.get("title", ""),
                    url=url,
                    snippet=item.get("snippet", ""),
                    position=i + 1,
                ))

            self._stats.searches_performed += 1
            logger.debug(f"Web search: '{query}' â†’ {len(results)} results")

        except Exception as e:
            error_msg = f"Search error: {e}"
            self._stats.errors.append(error_msg)
            logger.warning(error_msg)

        return results

    async def search_and_extract(self, query: str, max_pages: int = 3) -> list[ExtractedData]:
        """
        ĞŸĞ¾Ğ¸ÑĞº + Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¿ĞµÑ€Ğ²Ñ‹Ñ… N Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ².

        Args:
            query: ĞŸĞ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ğ¹ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ
            max_pages: Ğ¡ĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ† Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚ÑŒ

        Returns:
            Ğ¡Ğ¿Ğ¸ÑĞ¾Ğº ExtractedData
        """
        results = await self.web_search(query, max_results=max_pages + 2)
        extracted: list[ExtractedData] = []

        for result in results[:max_pages]:
            if not result.url or "duckduckgo.com" in result.url:
                continue

            try:
                await self._human.thinking_pause()
                data = await self.extract_data(result.url)
                extracted.append(data)
            except Exception as e:
                logger.debug(f"Extract failed for {result.url}: {e}")

        return extracted

    # â”€â”€â”€ Cookie / Session Management â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    async def save_cookies(self, path: str | Path | None = None) -> Path:
        """Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ cookies Ğ² Ñ„Ğ°Ğ¹Ğ»."""
        if not self._context:
            raise RuntimeError("Browser not started")

        if not path:
            path = self._cfg.downloads_dir / "cookies.json"

        path = Path(path)
        cookies = await self._context.cookies()
        path.write_text(json.dumps(cookies, indent=2))
        logger.debug(f"Cookies saved: {path} ({len(cookies)} cookies)")
        return path

    async def load_cookies(self, path: str | Path) -> int:
        """Ğ—Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚ÑŒ cookies Ğ¸Ğ· Ñ„Ğ°Ğ¹Ğ»Ğ°."""
        if not self._context:
            raise RuntimeError("Browser not started")

        path = Path(path)
        if not path.exists():
            logger.warning(f"Cookie file not found: {path}")
            return 0

        cookies = json.loads(path.read_text())
        await self._context.add_cookies(cookies)
        logger.debug(f"Cookies loaded: {len(cookies)} cookies")
        return len(cookies)

    async def clear_cookies(self) -> None:
        """ĞÑ‡Ğ¸ÑÑ‚Ğ¸Ñ‚ÑŒ Ğ²ÑĞµ cookies."""
        if self._context:
            await self._context.clear_cookies()

    # â”€â”€â”€ Storage State â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    async def save_session(self, path: str | Path | None = None) -> Path:
        """Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ ÑĞµÑÑĞ¸Ğ¸ (cookies + localStorage)."""
        if not self._context:
            raise RuntimeError("Browser not started")

        if not path:
            path = self._cfg.downloads_dir / "session_state.json"

        path = Path(path)
        await self._context.storage_state(path=str(path))
        logger.debug(f"Session state saved: {path}")
        return path

    # â”€â”€â”€ Download â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    async def download_file(self, url: str,
                            filename: str | None = None) -> Path | None:
        """
        Ğ¡ĞºĞ°Ñ‡Ğ°Ñ‚ÑŒ Ñ„Ğ°Ğ¹Ğ».

        Args:
            url: URL Ñ„Ğ°Ğ¹Ğ»Ğ°
            filename: Ğ˜Ğ¼Ñ Ñ„Ğ°Ğ¹Ğ»Ğ° (Ğ¸Ğ»Ğ¸ auto Ğ¸Ğ· URL)

        Returns:
            Path Ğº ÑĞºĞ°Ñ‡Ğ°Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ñ„Ğ°Ğ¹Ğ»Ñƒ Ğ¸Ğ»Ğ¸ None
        """
        if not self._page:
            raise RuntimeError("Browser not started")

        try:
            async with self._page.expect_download() as download_info:
                await self._page.goto(url)

            download = await download_info.value

            if not filename:
                filename = download.suggested_filename or f"download_{int(time.time())}"

            save_path = self._cfg.downloads_dir / filename
            await download.save_as(str(save_path))

            size = save_path.stat().st_size
            self._stats.total_bytes_downloaded += size
            logger.debug(f"Downloaded: {save_path} ({size} bytes)")
            return save_path

        except Exception as e:
            logger.warning(f"Download error: {url} â€” {e}")
            return None

    # â”€â”€â”€ Multi-Page â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    async def new_page(self, page_id: str | None = None) -> str:
        """Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ Ğ½Ğ¾Ğ²ÑƒÑ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñƒ (Ğ²ĞºĞ»Ğ°Ğ´ĞºÑƒ)."""
        if not self._context:
            raise RuntimeError("Browser not started")

        if len(self._pages) >= self._cfg.max_pages:
            # Ğ—Ğ°ĞºÑ€Ñ‹Ğ²Ğ°ĞµĞ¼ ÑĞ°Ğ¼ÑƒÑ ÑÑ‚Ğ°Ñ€ÑƒÑ
            oldest_id = next(iter(self._pages))
            await self.close_page(oldest_id)

        page = await self._context.new_page()
        page.set_default_timeout(self._cfg.default_timeout)

        if self._cfg.stealth_enabled:
            # stealth ÑƒĞ¶Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ñ‘Ğ½ Ñ‡ĞµÑ€ĞµĞ· context init_script
            pass

        pid = page_id or f"page_{len(self._pages) + 1}"
        self._pages[pid] = page
        return pid

    async def switch_page(self, page_id: str) -> bool:
        """ĞŸĞµÑ€ĞµĞºĞ»ÑÑ‡Ğ¸Ñ‚ÑŒÑÑ Ğ½Ğ° ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñƒ."""
        if page_id in self._pages:
            self._page = self._pages[page_id]
            return True
        return False

    async def close_page(self, page_id: str) -> None:
        """Ğ—Ğ°ĞºÑ€Ñ‹Ñ‚ÑŒ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñƒ."""
        if page_id in self._pages:
            try:
                await self._pages[page_id].close()
            except Exception:
                pass
            del self._pages[page_id]

    # â”€â”€â”€ Convenience â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    async def get_page_info(self) -> PageInfo:
        """ĞŸĞ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ñ‚ĞµĞºÑƒÑ‰ĞµĞ¹ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ğµ."""
        if not self._page:
            return PageInfo(url="", status=PageStatus.CLOSED)

        title = await self._page.title()
        self._last_title = title
        return PageInfo(
            url=self._page.url,
            title=title,
            status=PageStatus.READY,
        )

    async def scroll_down(self, distance: int = 500) -> None:
        """ĞŸÑ€Ğ¾ĞºÑ€ÑƒÑ‚ĞºĞ° Ğ²Ğ½Ğ¸Ğ·."""
        if self._page:
            await self._human.smooth_scroll(self._page, "down", distance)

    async def scroll_up(self, distance: int = 500) -> None:
        """ĞŸÑ€Ğ¾ĞºÑ€ÑƒÑ‚ĞºĞ° Ğ²Ğ²ĞµÑ€Ñ…."""
        if self._page:
            await self._human.smooth_scroll(self._page, "up", distance)

    async def press_key(self, key: str) -> None:
        """ĞĞ°Ğ¶Ğ°Ñ‚ÑŒ ĞºĞ»Ğ°Ğ²Ğ¸ÑˆÑƒ (Enter, Escape, Tab, etc)."""
        if self._page:
            await self._page.keyboard.press(key)

    async def get_html(self, selector: str | None = None) -> str:
        """ĞŸĞ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ HTML ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ğ¸Ğ»Ğ¸ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ°."""
        if not self._page:
            return ""
        try:
            if selector:
                return await self._page.locator(selector).first.inner_html()
            return await self._page.content()
        except Exception:
            return ""

    def get_stats_summary(self) -> str:
        """Ğ¢ĞµĞºÑÑ‚Ğ¾Ğ²Ğ°Ñ ÑĞ²Ğ¾Ğ´ĞºĞ° ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ¸."""
        s = self._stats
        return (
            f"ğŸŒ Browser Stats:\n"
            f"  Pages loaded: {s.pages_loaded}\n"
            f"  Pages failed: {s.pages_failed}\n"
            f"  Screenshots: {s.screenshots_taken}\n"
            f"  Searches: {s.searches_performed}\n"
            f"  Downloaded: {s.total_bytes_downloaded} bytes\n"
            f"  Errors: {len(s.errors)}"
        )

    # â”€â”€â”€ Context Manager â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    async def __aenter__(self) -> "BrowserEngine":
        await self.start()
        return self

    async def __aexit__(self, *args) -> None:
        await self.stop()


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 5. GLOBAL INSTANCE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

browser_engine = BrowserEngine()
