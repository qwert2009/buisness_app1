"""
PDS-Ultimate Translator (Language Bridge)
=============================================
–Ø–∑—ã–∫–æ–≤–æ–π –º–æ—Å—Ç: –∞–≤—Ç–æ-–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–∞, –ø–µ—Ä–µ–≤–æ–¥ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏,
–ø–∞–∫–µ—Ç–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥ –¥–ª—è WhatsApp-—Å–æ–æ–±—â–µ–Ω–∏–π.

–ü–æ –¢–ó ¬ß7.2:
- –ê–≤—Ç–æ-–ø–µ—Ä–µ–≤–æ–¥ WhatsApp: –ö–∏—Ç–∞–π—Å–∫–∏–π ‚Üî –†—É—Å—Å–∫–∏–π ‚Üî –ê–Ω–≥–ª–∏–π—Å–∫–∏–π ‚Üî –¢—É—Ä–∫–º–µ–Ω—Å–∫–∏–π
- –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏—è
- –ü–∞–∫–µ—Ç–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥ (—Å–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π)
- –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–µ—Ä–µ–≤–æ–¥–æ–≤
- –°–ª–æ–≤–∞—Ä—å –±–∏–∑–Ω–µ—Å-—Ç–µ—Ä–º–∏–Ω–æ–≤ (–ª–æ–≥–∏—Å—Ç–∏–∫–∞, —Ç–æ—Ä–≥–æ–≤–ª—è)

–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —è–∑—ã–∫–∏:
    ru (–†—É—Å—Å–∫–∏–π), en (English), zh (‰∏≠Êñá),
    tk (T√ºrkmen), tr (T√ºrk√ße), ar (ÿßŸÑÿπÿ±ÿ®Ÿäÿ©)
"""

from __future__ import annotations

import hashlib
import re
import time
from collections import OrderedDict
from dataclasses import dataclass, field
from typing import Optional

from pds_ultimate.config import logger

# ‚îÄ‚îÄ‚îÄ Data Models ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

LANGUAGE_NAMES = {
    "ru": "–†—É—Å—Å–∫–∏–π",
    "en": "English",
    "zh": "‰∏≠Êñá",
    "tk": "T√ºrkmen",
    "tr": "T√ºrk√ße",
    "ar": "ÿßŸÑÿπÿ±ÿ®Ÿäÿ©",
    "de": "Deutsch",
    "fr": "Fran√ßais",
    "es": "Espa√±ol",
    "pt": "Portugu√™s",
    "ja": "Êó•Êú¨Ë™û",
    "ko": "ÌïúÍµ≠Ïñ¥",
}


@dataclass
class TranslationResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –ø–µ—Ä–µ–≤–æ–¥–∞."""
    original: str
    translated: str
    source_lang: str
    target_lang: str
    detected_lang: Optional[str] = None
    confidence: float = 0.0
    cached: bool = False
    translation_time_ms: float = 0.0

    def to_dict(self) -> dict:
        return {
            "original": self.original,
            "translated": self.translated,
            "source_lang": self.source_lang,
            "target_lang": self.target_lang,
            "detected_lang": self.detected_lang,
            "confidence": round(self.confidence, 3),
            "cached": self.cached,
            "time_ms": round(self.translation_time_ms, 1),
        }


@dataclass
class BatchTranslation:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –ø–∞–∫–µ—Ç–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞."""
    results: list[TranslationResult] = field(default_factory=list)
    total_time_ms: float = 0.0
    from_cache: int = 0
    from_api: int = 0

    @property
    def success_count(self) -> int:
        return len([r for r in self.results if r.translated])

    def to_dict(self) -> dict:
        return {
            "count": len(self.results),
            "success": self.success_count,
            "from_cache": self.from_cache,
            "from_api": self.from_api,
            "total_time_ms": round(self.total_time_ms, 1),
        }


# ‚îÄ‚îÄ‚îÄ Language Detection ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

# Unicode ranges for language detection
LANG_RANGES = {
    "zh": [
        (0x4E00, 0x9FFF),   # CJK Unified Ideographs
        (0x3400, 0x4DBF),   # CJK Extension A
        (0xF900, 0xFAFF),   # CJK Compatibility
    ],
    "ru": [
        (0x0400, 0x04FF),   # Cyrillic
        (0x0500, 0x052F),   # Cyrillic Supplement
    ],
    "ar": [
        (0x0600, 0x06FF),   # Arabic
        (0x0750, 0x077F),   # Arabic Supplement
    ],
    "ja": [
        (0x3040, 0x309F),   # Hiragana
        (0x30A0, 0x30FF),   # Katakana
    ],
    "ko": [
        (0xAC00, 0xD7AF),   # Hangul Syllables
    ],
}

# Turkmen-specific characters (Latin with diacritics)
TURKMEN_CHARS = set("√Ñ√§√á√ß≈á≈à√ñ√∂≈û≈ü√ú√º√ù√Ω≈Ω≈æ")


class LanguageDetector:
    """
    –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–∞ —Ç–µ–∫—Å—Ç–∞ –ø–æ Unicode-–¥–∏–∞–ø–∞–∑–æ–Ω–∞–º –∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω—ã–º –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º.

    –ë—ã—Å—Ç—Ä—ã–π offline-–¥–µ—Ç–µ–∫—Ç–æ—Ä –±–µ–∑ API:
    - –ê–Ω–∞–ª–∏–∑ Unicode-—Å–∏–º–≤–æ–ª–æ–≤
    - –•–∞—Ä–∞–∫—Ç–µ—Ä–Ω—ã–µ –±—É–∫–≤—ã –¥–ª—è —Ç—É—Ä–∫–º–µ–Ω—Å–∫–æ–≥–æ/—Ç—É—Ä–µ—Ü–∫–æ–≥–æ
    - –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–¥—Å—á—ë—Ç
    """

    def detect(self, text: str) -> tuple[str, float]:
        """
        –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —è–∑—ã–∫ —Ç–µ–∫—Å—Ç–∞.
        Returns: (language_code, confidence)
        """
        if not text or not text.strip():
            return ("en", 0.0)

        # –ü–æ–¥—Å—á—ë—Ç —Å–∏–º–≤–æ–ª–æ–≤ –ø–æ –¥–∏–∞–ø–∞–∑–æ–Ω–∞–º
        scores: dict[str, int] = {}
        total_alpha = 0

        for char in text:
            cp = ord(char)

            if not char.isalpha() and not (0x4E00 <= cp <= 0x9FFF):
                continue

            total_alpha += 1

            for lang, ranges in LANG_RANGES.items():
                for start, end in ranges:
                    if start <= cp <= end:
                        scores[lang] = scores.get(lang, 0) + 1
                        break

            # Latin characters
            if char.isascii() and char.isalpha():
                scores.setdefault("latin", 0)
                scores["latin"] = scores.get("latin", 0) + 1

        if total_alpha == 0:
            return ("en", 0.0)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç—É—Ä–∫–º–µ–Ω—Å–∫–∏–π (latin + —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–µ —Å–∏–º–≤–æ–ª—ã)
        if "latin" in scores and scores["latin"] > total_alpha * 0.5:
            turkmen_count = sum(1 for c in text if c in TURKMEN_CHARS)
            if turkmen_count >= 2:
                confidence = min(
                    turkmen_count / max(total_alpha, 1) * 10, 0.95)
                return ("tk", confidence)

            # –¢—É—Ä–µ—Ü–∫–∏–π (similar to Turkmen but with ƒ∞, ƒ±, ƒü)
            turkish_chars = set("ƒ∞ƒ±ƒûƒü")
            turkish_count = sum(1 for c in text if c in turkish_chars)
            if turkish_count >= 2:
                confidence = min(turkish_count / max(total_alpha, 1) * 10, 0.9)
                return ("tr", confidence)

        # –í—ã–±–∏—Ä–∞–µ–º —è–∑—ã–∫ —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º score
        if scores:
            # –£–±–∏—Ä–∞–µ–º "latin" ‚Äî —ç—Ç–æ fallback
            lang_scores = {k: v for k, v in scores.items() if k != "latin"}

            if lang_scores:
                best_lang = max(lang_scores, key=lang_scores.get)
                confidence = lang_scores[best_lang] / total_alpha
                return (best_lang, min(confidence, 0.99))

            # –¢–æ–ª—å–∫–æ latin ‚Üí English –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
            if "latin" in scores:
                return ("en", scores["latin"] / total_alpha)

        return ("en", 0.3)


# ‚îÄ‚îÄ‚îÄ Business Glossary ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

BUSINESS_GLOSSARY: dict[str, dict[str, str]] = {
    # RU ‚Üí ZH
    "–ø–æ—Å—Ç–∞–≤—â–∏–∫": {"zh": "‰æõÂ∫îÂïÜ", "en": "supplier"},
    "–∑–∞–∫–∞–∑": {"zh": "ËÆ¢Âçï", "en": "order"},
    "–¥–æ—Å—Ç–∞–≤–∫–∞": {"zh": "ÂèëË¥ß", "en": "delivery"},
    "–æ–ø–ª–∞—Ç–∞": {"zh": "‰ªòÊ¨æ", "en": "payment"},
    "—Ü–µ–Ω–∞": {"zh": "‰ª∑Ê†º", "en": "price"},
    "—Ç–æ–≤–∞—Ä": {"zh": "ÂïÜÂìÅ", "en": "goods"},
    "—Å–∫–ª–∞–¥": {"zh": "‰ªìÂ∫ì", "en": "warehouse"},
    "–∫–æ–Ω—Ç–µ–π–Ω–µ—Ä": {"zh": "ÈõÜË£ÖÁÆ±", "en": "container"},
    "–Ω–∞–∫–ª–∞–¥–Ω–∞—è": {"zh": "ËøêÂçï", "en": "waybill"},
    "—Ç–∞–º–æ–∂–Ω—è": {"zh": "Êµ∑ÂÖ≥", "en": "customs"},
    "—Ç—Ä–µ–∫-–Ω–æ–º–µ—Ä": {"zh": "Âø´ÈÄíÂçïÂè∑", "en": "tracking number"},
    "–æ–±—Ä–∞–∑–µ—Ü": {"zh": "Ê†∑ÂìÅ", "en": "sample"},
    "–∫–∞—á–µ—Å—Ç–≤–æ": {"zh": "Ë¥®Èáè", "en": "quality"},
    "–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ": {"zh": "Êï∞Èáè", "en": "quantity"},
    "–≤–µ—Å": {"zh": "ÈáçÈáè", "en": "weight"},
    "–æ–±—ä—ë–º": {"zh": "‰ΩìÁßØ", "en": "volume"},
    "—É–ø–∞–∫–æ–≤–∫–∞": {"zh": "ÂåÖË£Ö", "en": "packaging"},
    "—Ñ–∞–±—Ä–∏–∫–∞": {"zh": "Â∑•ÂéÇ", "en": "factory"},
    "—Ç–æ—Ä–≥–æ–≤–ª—è": {"zh": "Ë¥∏Êòì", "en": "trade"},
    "–ø—Ä–∏–±—ã–ª—å": {"zh": "Âà©Ê∂¶", "en": "profit"},
}


# ‚îÄ‚îÄ‚îÄ Translation Cache ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

class TranslationCache:
    """LRU-–∫—ç—à –ø–µ—Ä–µ–≤–æ–¥–æ–≤."""

    def __init__(self, max_size: int = 1000):
        self._cache: OrderedDict[str, TranslationResult] = OrderedDict()
        self._max_size = max_size
        self._hits = 0
        self._misses = 0

    def _key(self, text: str, source: str, target: str) -> str:
        h = hashlib.md5(text.encode()).hexdigest()[:12]
        return f"{source}:{target}:{h}"

    def get(
        self, text: str, source: str, target: str,
    ) -> Optional[TranslationResult]:
        key = self._key(text, source, target)
        if key in self._cache:
            self._hits += 1
            self._cache.move_to_end(key)
            result = self._cache[key]
            result.cached = True
            return result
        self._misses += 1
        return None

    def put(self, result: TranslationResult) -> None:
        key = self._key(result.original, result.source_lang,
                        result.target_lang)
        self._cache[key] = result
        if len(self._cache) > self._max_size:
            self._cache.popitem(last=False)

    @property
    def stats(self) -> dict:
        total = self._hits + self._misses
        return {
            "size": len(self._cache),
            "max_size": self._max_size,
            "hits": self._hits,
            "misses": self._misses,
            "hit_rate": (
                round(self._hits / total, 3) if total > 0 else 0
            ),
        }


# ‚îÄ‚îÄ‚îÄ Translator Service ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

class TranslatorService:
    """
    –Ø–∑—ã–∫–æ–≤–æ–π –º–æ—Å—Ç: –ø–µ—Ä–µ–≤–æ–¥, –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–∞, –±–∏–∑–Ω–µ—Å-–≥–ª–æ—Å—Å–∞—Ä–∏–π.

    –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:
    - LanguageDetector: offline –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–∞
    - DeepSeek LLM: –æ—Å–Ω–æ–≤–Ω–æ–π –¥–≤–∏–∂–æ–∫ –ø–µ—Ä–µ–≤–æ–¥–∞
    - TranslationCache: LRU-–∫—ç—à —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    - Business Glossary: —Å–ª–æ–≤–∞—Ä—å —Ç–æ—Ä–≥–æ–≤—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤
    - Batch API: –ø–∞–∫–µ—Ç–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥

    –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
        result = await translator.translate("–ü—Ä–∏–≤–µ—Ç", target_lang="en")
        lang, conf = translator.detect_language("‰Ω†Â•Ω")
        batch = await translator.translate_batch(messages, target_lang="ru")
    """

    def __init__(self, cache_size: int = 1000):
        self._detector = LanguageDetector()
        self._cache = TranslationCache(max_size=cache_size)
        self._glossary = BUSINESS_GLOSSARY
        self._translation_count = 0
        self._total_chars = 0

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # Public API
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def detect_language(self, text: str) -> tuple[str, float]:
        """–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —è–∑—ã–∫ —Ç–µ–∫—Å—Ç–∞."""
        return self._detector.detect(text)

    def get_language_name(self, code: str) -> str:
        """–ü–æ–ª—É—á–∏—Ç—å –Ω–∞–∑–≤–∞–Ω–∏–µ —è–∑—ã–∫–∞ –ø–æ –∫–æ–¥—É."""
        return LANGUAGE_NAMES.get(code, code)

    async def translate(
        self,
        text: str,
        target_lang: str = "ru",
        source_lang: Optional[str] = None,
    ) -> TranslationResult:
        """
        –ü–µ—Ä–µ–≤–µ—Å—Ç–∏ —Ç–µ–∫—Å—Ç.

        Args:
            text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
            target_lang: –¶–µ–ª–µ–≤–æ–π —è–∑—ã–∫ (ru, en, zh, tk, tr, ar)
            source_lang: –ò—Å—Ö–æ–¥–Ω—ã–π —è–∑—ã–∫ (auto-detect –µ—Å–ª–∏ None)
        """
        start = time.monotonic()

        if not text or not text.strip():
            return TranslationResult(
                original=text,
                translated=text,
                source_lang=source_lang or "?",
                target_lang=target_lang,
            )

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —è–∑—ã–∫ –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω
        detected = None
        if not source_lang:
            source_lang, det_conf = self.detect_language(text)
            detected = source_lang

        # –û–¥–∏–Ω–∞–∫–æ–≤—ã–π —è–∑—ã–∫ ‚Äî –≤–æ–∑–≤—Ä–∞—Ç
        if source_lang == target_lang:
            return TranslationResult(
                original=text,
                translated=text,
                source_lang=source_lang,
                target_lang=target_lang,
                detected_lang=detected,
                confidence=1.0,
            )

        # –ö—ç—à
        cached = self._cache.get(text, source_lang, target_lang)
        if cached:
            cached.translation_time_ms = (
                time.monotonic() - start
            ) * 1000
            return cached

        # –ü–µ—Ä–µ–≤–æ–¥ —á–µ—Ä–µ–∑ LLM
        translated = await self._translate_llm(
            text, source_lang, target_lang
        )

        elapsed = (time.monotonic() - start) * 1000
        self._translation_count += 1
        self._total_chars += len(text)

        result = TranslationResult(
            original=text,
            translated=translated,
            source_lang=source_lang,
            target_lang=target_lang,
            detected_lang=detected,
            confidence=0.9 if translated else 0.0,
            translation_time_ms=elapsed,
        )

        # –ö—ç—à–∏—Ä–æ–≤–∞—Ç—å
        if translated:
            self._cache.put(result)

        return result

    async def translate_batch(
        self,
        texts: list[str],
        target_lang: str = "ru",
        source_lang: Optional[str] = None,
    ) -> BatchTranslation:
        """
        –ü–∞–∫–µ—Ç–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥ —Å–ø–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤.
        –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç: –∫—ç—à + –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–ª—è –æ–¥–Ω–æ–≥–æ LLM-–∑–∞–ø—Ä–æ—Å–∞.
        """
        start = time.monotonic()
        results = []
        from_cache = 0
        from_api = 0
        to_translate = []

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à –¥–ª—è –∫–∞–∂–¥–æ–≥–æ
        for text in texts:
            src = source_lang
            if not src:
                src, _ = self.detect_language(text)

            cached = self._cache.get(text, src, target_lang)
            if cached:
                results.append(cached)
                from_cache += 1
            else:
                to_translate.append((text, src))
                results.append(None)  # placeholder

        # –ü–µ—Ä–µ–≤–æ–¥–∏–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è
        if to_translate:
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –¥–ª—è –æ–¥–Ω–æ–≥–æ LLM-–∑–∞–ø—Ä–æ—Å–∞ (–µ—Å–ª–∏ < 10)
            if len(to_translate) <= 10:
                translations = await self._translate_batch_llm(
                    to_translate, target_lang
                )
            else:
                # –ü–æ –æ–¥–Ω–æ–º—É –¥–ª—è –±–æ–ª—å—à–∏—Ö –ø–∞–∫–µ—Ç–æ–≤
                translations = []
                for text, src in to_translate:
                    t = await self._translate_llm(text, src, target_lang)
                    translations.append(t)

            # –ó–∞–ø–æ–ª–Ω—è–µ–º placeholders
            t_idx = 0
            for i, r in enumerate(results):
                if r is None:
                    text, src = to_translate[t_idx]
                    translated = (
                        translations[t_idx]
                        if t_idx < len(translations) else ""
                    )
                    result = TranslationResult(
                        original=text,
                        translated=translated,
                        source_lang=src,
                        target_lang=target_lang,
                        confidence=0.9 if translated else 0.0,
                    )
                    if translated:
                        self._cache.put(result)
                    results[i] = result
                    from_api += 1
                    t_idx += 1

        elapsed = (time.monotonic() - start) * 1000

        return BatchTranslation(
            results=results,
            total_time_ms=elapsed,
            from_cache=from_cache,
            from_api=from_api,
        )

    def lookup_glossary(
        self,
        term: str,
        target_lang: str = "zh",
    ) -> Optional[str]:
        """–ù–∞–π—Ç–∏ –±–∏–∑–Ω–µ—Å-—Ç–µ—Ä–º–∏–Ω –≤ –≥–ª–æ—Å—Å–∞—Ä–∏–∏."""
        term_lower = term.lower().strip()
        entry = self._glossary.get(term_lower)
        if entry:
            return entry.get(target_lang)
        return None

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # Formatting
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def format_translation(self, result: TranslationResult) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è –±–æ—Ç–∞."""
        src_name = self.get_language_name(result.source_lang)
        tgt_name = self.get_language_name(result.target_lang)

        lines = [f"üåê –ü–µ—Ä–µ–≤–æ–¥ ({src_name} ‚Üí {tgt_name}):\n"]
        lines.append(f"  üìù {result.translated}")

        if result.detected_lang:
            det_name = self.get_language_name(result.detected_lang)
            lines.append(f"\n  üîç –û–ø—Ä–µ–¥–µ–ª—ë–Ω —è–∑—ã–∫: {det_name}")

        if result.cached:
            lines.append("  ‚ö° –ò–∑ –∫—ç—à–∞")

        return "\n".join(lines)

    def get_stats(self) -> dict:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫–∞."""
        return {
            "translations_total": self._translation_count,
            "total_chars": self._total_chars,
            "cache": self._cache.stats,
            "supported_languages": list(LANGUAGE_NAMES.keys()),
            "glossary_size": len(self._glossary),
        }

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # Internal: LLM Translation
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    async def _translate_llm(
        self,
        text: str,
        source_lang: str,
        target_lang: str,
    ) -> str:
        """–ü–µ—Ä–µ–≤–æ–¥ —á–µ—Ä–µ–∑ DeepSeek LLM."""
        try:
            from pds_ultimate.core.llm_engine import llm_engine

            src_name = self.get_language_name(source_lang)
            tgt_name = self.get_language_name(target_lang)

            prompt = (
                f"–ü–µ—Ä–µ–≤–µ–¥–∏ —Å–ª–µ–¥—É—é—â–∏–π —Ç–µ–∫—Å—Ç —Å {src_name} –Ω–∞ {tgt_name}. "
                f"–í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û –ø–µ—Ä–µ–≤–æ–¥, –±–µ–∑ –æ–±—ä—è—Å–Ω–µ–Ω–∏–π –∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤.\n\n"
                f"–¢–µ–∫—Å—Ç: {text}"
            )

            result = await llm_engine.chat(
                message=prompt,
                task_type="translate",
                temperature=0.1,
            )

            return result.strip() if result else ""

        except Exception as e:
            logger.warning(f"[Translator] LLM translation failed: {e}")
            return ""

    async def _translate_batch_llm(
        self,
        texts_with_langs: list[tuple[str, str]],
        target_lang: str,
    ) -> list[str]:
        """–ü–∞–∫–µ—Ç–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥ —á–µ—Ä–µ–∑ –æ–¥–∏–Ω LLM-–∑–∞–ø—Ä–æ—Å."""
        try:
            from pds_ultimate.core.llm_engine import llm_engine

            tgt_name = self.get_language_name(target_lang)

            numbered = []
            for i, (text, src) in enumerate(texts_with_langs, 1):
                src_name = self.get_language_name(src)
                numbered.append(f"{i}. [{src_name}] {text}")

            prompt = (
                f"–ü–µ—Ä–µ–≤–µ–¥–∏ –∫–∞–∂–¥–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –Ω–∞ {tgt_name}. "
                f"–í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û –ø–µ—Ä–µ–≤–æ–¥—ã, –ø–æ –æ–¥–Ω–æ–º—É –Ω–∞ —Å—Ç—Ä–æ–∫—É, "
                f"—Å –Ω–æ–º–µ—Ä–∞–º–∏:\n\n" + "\n".join(numbered)
            )

            result = await llm_engine.chat(
                message=prompt,
                task_type="translate",
                temperature=0.1,
            )

            if not result:
                return [""] * len(texts_with_langs)

            # –ü–∞—Ä—Å–∏–º –Ω—É–º–µ—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫
            lines = result.strip().split("\n")
            translations = []

            for line in lines:
                # Remove leading number: "1. text" ‚Üí "text"
                cleaned = re.sub(r'^\d+[.)]\s*', '', line.strip())
                if cleaned:
                    translations.append(cleaned)

            # –î–æ–ø–æ–ª–Ω—è–µ–º –µ—Å–ª–∏ –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç
            while len(translations) < len(texts_with_langs):
                translations.append("")

            return translations[:len(texts_with_langs)]

        except Exception as e:
            logger.warning(f"[Translator] Batch LLM failed: {e}")
            return [""] * len(texts_with_langs)


# ‚îÄ‚îÄ‚îÄ –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

translator = TranslatorService()
